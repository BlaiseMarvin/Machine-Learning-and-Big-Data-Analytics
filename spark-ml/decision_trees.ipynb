{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d854f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import pyspark.sql.functions as f\n",
    "import os, sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968a999f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x1d1d8aeba00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\",\"trees\")\n",
    "conf.set(\"spark.master\",\"local[*]\")\n",
    "conf.set(\"spark.driver.memory\",\"8g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d99a67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                    .config(conf=conf)\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa378106",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_header = spark.read\\\n",
    "                           .format(\"csv\")\\\n",
    "                           .option(\"header\",False)\\\n",
    "                           .option(\"inferSchema\",True)\\\n",
    "                           .load(r\"C:\\Users\\blais\\Documents\\ML\\data\\covertype\\covtype.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d471c252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: integer (nullable = true)\n",
      " |-- _c15: integer (nullable = true)\n",
      " |-- _c16: integer (nullable = true)\n",
      " |-- _c17: integer (nullable = true)\n",
      " |-- _c18: integer (nullable = true)\n",
      " |-- _c19: integer (nullable = true)\n",
      " |-- _c20: integer (nullable = true)\n",
      " |-- _c21: integer (nullable = true)\n",
      " |-- _c22: integer (nullable = true)\n",
      " |-- _c23: integer (nullable = true)\n",
      " |-- _c24: integer (nullable = true)\n",
      " |-- _c25: integer (nullable = true)\n",
      " |-- _c26: integer (nullable = true)\n",
      " |-- _c27: integer (nullable = true)\n",
      " |-- _c28: integer (nullable = true)\n",
      " |-- _c29: integer (nullable = true)\n",
      " |-- _c30: integer (nullable = true)\n",
      " |-- _c31: integer (nullable = true)\n",
      " |-- _c32: integer (nullable = true)\n",
      " |-- _c33: integer (nullable = true)\n",
      " |-- _c34: integer (nullable = true)\n",
      " |-- _c35: integer (nullable = true)\n",
      " |-- _c36: integer (nullable = true)\n",
      " |-- _c37: integer (nullable = true)\n",
      " |-- _c38: integer (nullable = true)\n",
      " |-- _c39: integer (nullable = true)\n",
      " |-- _c40: integer (nullable = true)\n",
      " |-- _c41: integer (nullable = true)\n",
      " |-- _c42: integer (nullable = true)\n",
      " |-- _c43: integer (nullable = true)\n",
      " |-- _c44: integer (nullable = true)\n",
      " |-- _c45: integer (nullable = true)\n",
      " |-- _c46: integer (nullable = true)\n",
      " |-- _c47: integer (nullable = true)\n",
      " |-- _c48: integer (nullable = true)\n",
      " |-- _c49: integer (nullable = true)\n",
      " |-- _c50: integer (nullable = true)\n",
      " |-- _c51: integer (nullable = true)\n",
      " |-- _c52: integer (nullable = true)\n",
      " |-- _c53: integer (nullable = true)\n",
      " |-- _c54: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_without_header.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fefab43",
   "metadata": {},
   "source": [
    "Code reads the input as csv and doesn't attempt to parse the first line as a header of column names. It also requests that the type of each column be inferred by examining the data. It correctly infers that all of the columns are numbers, and, more specifically integers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece8e64d",
   "metadata": {},
   "source": [
    "We can look at the covtype.info file for the column names:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb9efc",
   "metadata": {},
   "source": [
    "In any event before proceeding, it is useful to add column names to this DataFrame to make it easier to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21d956b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a08c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = [\"Elevation\", \"Aspect\", \"Slope\", \\\n",
    "            \"Horizontal_Distance_To_Hydrology\", \\\n",
    "            \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\", \\\n",
    "            \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\", \\\n",
    "            \"Horizontal_Distance_To_Fire_Points\"] + \\\n",
    "           [f\"Wilderness_Area_{i}\" for i in range(4)] + \\\n",
    "           [f\"Soil_Type_{i}\" for i in range(40)] + \\\n",
    "           [\"Cover_Type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e2a1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_without_header.toDF(*colnames)\\\n",
    "                           .withColumn(\"Cover_Type\", f.col(\"Cover_Type\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba4d3d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Elevation=2596, Aspect=51, Slope=3, Horizontal_Distance_To_Hydrology=258, Vertical_Distance_To_Hydrology=0, Horizontal_Distance_To_Roadways=510, Hillshade_9am=221, Hillshade_Noon=232, Hillshade_3pm=148, Horizontal_Distance_To_Fire_Points=6279, Wilderness_Area_0=1, Wilderness_Area_1=0, Wilderness_Area_2=0, Wilderness_Area_3=0, Soil_Type_0=0, Soil_Type_1=0, Soil_Type_2=0, Soil_Type_3=0, Soil_Type_4=0, Soil_Type_5=0, Soil_Type_6=0, Soil_Type_7=0, Soil_Type_8=0, Soil_Type_9=0, Soil_Type_10=0, Soil_Type_11=0, Soil_Type_12=0, Soil_Type_13=0, Soil_Type_14=0, Soil_Type_15=0, Soil_Type_16=0, Soil_Type_17=0, Soil_Type_18=0, Soil_Type_19=0, Soil_Type_20=0, Soil_Type_21=0, Soil_Type_22=0, Soil_Type_23=0, Soil_Type_24=0, Soil_Type_25=0, Soil_Type_26=0, Soil_Type_27=0, Soil_Type_28=1, Soil_Type_29=0, Soil_Type_30=0, Soil_Type_31=0, Soil_Type_32=0, Soil_Type_33=0, Soil_Type_34=0, Soil_Type_35=0, Soil_Type_36=0, Soil_Type_37=0, Soil_Type_38=0, Soil_Type_39=0, Cover_Type=5.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef0aea",
   "metadata": {},
   "source": [
    "**Our First Decision Tree:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23bdc7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Elevation: int, Aspect: int, Slope: int, Horizontal_Distance_To_Hydrology: int, Vertical_Distance_To_Hydrology: int, Horizontal_Distance_To_Roadways: int, Hillshade_9am: int, Hillshade_Noon: int, Hillshade_3pm: int, Horizontal_Distance_To_Fire_Points: int, Wilderness_Area_0: int, Wilderness_Area_1: int, Wilderness_Area_2: int, Wilderness_Area_3: int, Soil_Type_0: int, Soil_Type_1: int, Soil_Type_2: int, Soil_Type_3: int, Soil_Type_4: int, Soil_Type_5: int, Soil_Type_6: int, Soil_Type_7: int, Soil_Type_8: int, Soil_Type_9: int, Soil_Type_10: int, Soil_Type_11: int, Soil_Type_12: int, Soil_Type_13: int, Soil_Type_14: int, Soil_Type_15: int, Soil_Type_16: int, Soil_Type_17: int, Soil_Type_18: int, Soil_Type_19: int, Soil_Type_20: int, Soil_Type_21: int, Soil_Type_22: int, Soil_Type_23: int, Soil_Type_24: int, Soil_Type_25: int, Soil_Type_26: int, Soil_Type_27: int, Soil_Type_28: int, Soil_Type_29: int, Soil_Type_30: int, Soil_Type_31: int, Soil_Type_32: int, Soil_Type_33: int, Soil_Type_34: int, Soil_Type_35: int, Soil_Type_36: int, Soil_Type_37: int, Soil_Type_38: int, Soil_Type_39: int, Cover_Type: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_data, test_data) = data.randomSplit([0.9,0.1])\n",
    "train_data.cache()\n",
    "test_data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20dc019",
   "metadata": {},
   "source": [
    "The data needs a little more preparation to be used with a classifier in MLlib. The input dataframe contains many columns, each holding one feature that could be used to predict the target column. MLlib requires all of the inputs to be collected into one column - whose value is a vector. PySpark's VectorAssembler class is an abstraction for vectors in the linear algebra sense and contains only numbers. For most intents and purposes, they work like a simple array of double values (floating-point numbers). Of course, some of the input features are conceptually categorical, even if they are represented with numbers in the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b37f65bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dd1efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = colnames[:-1]\n",
    "vector_assembler = VectorAssembler(inputCols=input_cols, outputCol=\"featureVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "615f3567",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembled_train_data = vector_assembler.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7181c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|featureVector                                                                                        |\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1863.0,37.0,17.0,120.0,18.0,90.0,217.0,202.0,115.0,769.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,5,6,7,8,9,13,18],[1874.0,18.0,14.0,90.0,208.0,209.0,135.0,793.0,1.0,1.0])                 |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1879.0,28.0,19.0,30.0,12.0,95.0,209.0,196.0,117.0,778.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1888.0,33.0,22.0,150.0,46.0,108.0,209.0,185.0,103.0,735.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1889.0,28.0,22.0,150.0,23.0,120.0,205.0,185.0,108.0,759.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1889.0,353.0,30.0,95.0,39.0,67.0,153.0,172.0,146.0,600.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1896.0,337.0,12.0,30.0,6.0,175.0,195.0,224.0,168.0,732.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1898.0,34.0,23.0,175.0,56.0,134.0,210.0,184.0,99.0,765.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1899.0,355.0,22.0,153.0,43.0,124.0,178.0,195.0,151.0,819.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1901.0,311.0,9.0,30.0,2.0,190.0,195.0,234.0,179.0,726.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1903.0,5.0,13.0,42.0,4.0,201.0,203.0,214.0,148.0,708.0,1.0,1.0])    |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[1903.0,67.0,16.0,108.0,36.0,120.0,234.0,207.0,100.0,969.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1904.0,51.0,26.0,67.0,30.0,162.0,222.0,175.0,72.0,711.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1905.0,19.0,27.0,134.0,58.0,120.0,188.0,171.0,108.0,636.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1905.0,33.0,27.0,90.0,46.0,150.0,204.0,171.0,89.0,725.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1906.0,356.0,20.0,150.0,55.0,120.0,184.0,201.0,151.0,726.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1908.0,323.0,32.0,150.0,52.0,120.0,125.0,190.0,196.0,765.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1916.0,24.0,25.0,212.0,74.0,175.0,197.0,177.0,105.0,789.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1916.0,320.0,24.0,190.0,60.0,162.0,151.0,210.0,195.0,832.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,23],[1918.0,321.0,28.0,42.0,17.0,85.0,139.0,201.0,196.0,402.0,1.0,1.0])  |\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembled_train_data.select(\"featureVector\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3f5aac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+--------------------------------+------------------------------+-------------------------------+-------------+--------------+-------------+----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+----------+--------------------+\n",
      "|Elevation|Aspect|Slope|Horizontal_Distance_To_Hydrology|Vertical_Distance_To_Hydrology|Horizontal_Distance_To_Roadways|Hillshade_9am|Hillshade_Noon|Hillshade_3pm|Horizontal_Distance_To_Fire_Points|Wilderness_Area_0|Wilderness_Area_1|Wilderness_Area_2|Wilderness_Area_3|Soil_Type_0|Soil_Type_1|Soil_Type_2|Soil_Type_3|Soil_Type_4|Soil_Type_5|Soil_Type_6|Soil_Type_7|Soil_Type_8|Soil_Type_9|Soil_Type_10|Soil_Type_11|Soil_Type_12|Soil_Type_13|Soil_Type_14|Soil_Type_15|Soil_Type_16|Soil_Type_17|Soil_Type_18|Soil_Type_19|Soil_Type_20|Soil_Type_21|Soil_Type_22|Soil_Type_23|Soil_Type_24|Soil_Type_25|Soil_Type_26|Soil_Type_27|Soil_Type_28|Soil_Type_29|Soil_Type_30|Soil_Type_31|Soil_Type_32|Soil_Type_33|Soil_Type_34|Soil_Type_35|Soil_Type_36|Soil_Type_37|Soil_Type_38|Soil_Type_39|Cover_Type|       featureVector|\n",
      "+---------+------+-----+--------------------------------+------------------------------+-------------------------------+-------------+--------------+-------------+----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+----------+--------------------+\n",
      "|     1863|    37|   17|                             120|                            18|                             90|          217|           202|          115|                               769|                0|                0|                0|                1|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1874|    18|   14|                               0|                             0|                             90|          208|           209|          135|                               793|                0|                0|                0|                1|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,5,6,7,...|\n",
      "|     1879|    28|   19|                              30|                            12|                             95|          209|           196|          117|                               778|                0|                0|                0|                1|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1888|    33|   22|                             150|                            46|                            108|          209|           185|          103|                               735|                0|                0|                0|                1|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1889|    28|   22|                             150|                            23|                            120|          205|           185|          108|                               759|                0|                0|                0|                1|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1889|   353|   30|                              95|                            39|                             67|          153|           172|          146|                               600|                0|                0|                0|                1|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1896|   337|   12|                              30|                             6|                            175|          195|           224|          168|                               732|                0|                0|                0|                1|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1898|    34|   23|                             175|                            56|                            134|          210|           184|           99|                               765|                0|                0|                0|                1|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1899|   355|   22|                             153|                            43|                            124|          178|           195|          151|                               819|                0|                0|                0|                1|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1901|   311|    9|                              30|                             2|                            190|          195|           234|          179|                               726|                0|                0|                0|                1|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1903|     5|   13|                              42|                             4|                            201|          203|           214|          148|                               708|                0|                0|                0|                1|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1903|    67|   16|                             108|                            36|                            120|          234|           207|          100|                               969|                0|                0|                0|                1|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       3.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1904|    51|   26|                              67|                            30|                            162|          222|           175|           72|                               711|                0|                0|                0|                1|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1905|    19|   27|                             134|                            58|                            120|          188|           171|          108|                               636|                0|                0|                0|                1|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1905|    33|   27|                              90|                            46|                            150|          204|           171|           89|                               725|                0|                0|                0|                1|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1906|   356|   20|                             150|                            55|                            120|          184|           201|          151|                               726|                0|                0|                0|                1|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1908|   323|   32|                             150|                            52|                            120|          125|           190|          196|                               765|                0|                0|                0|                1|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1916|    24|   25|                             212|                            74|                            175|          197|           177|          105|                               789|                0|                0|                0|                1|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1916|   320|   24|                             190|                            60|                            162|          151|           210|          195|                               832|                0|                0|                0|                1|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       6.0|(54,[0,1,2,3,4,5,...|\n",
      "|     1918|   321|   28|                              42|                            17|                             85|          139|           201|          196|                               402|                0|                0|                0|                1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       3.0|(54,[0,1,2,3,4,5,...|\n",
      "+---------+------+-----+--------------------------------+------------------------------+-------------------------------+-------------+--------------+-------------+----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembled_train_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a710a1",
   "metadata": {},
   "source": [
    "VectorAssembler is an example of a Transformer within the current MLlib Pipelines API. It transforms the input DataFrame into another DataFrame based on some logic, and is composable with other transformations into a pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95b9d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee9adf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(seed=1234, labelCol=\"Cover_Type\",\n",
    "                                    featuresCol=\"featureVector\",\n",
    "                                    predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c720a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classifier.fit(assembled_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b66ba0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_a5c5340a56e9, depth=5, numNodes=41, numClasses=8, numFeatures=54\n",
      "  If (feature 0 <= 3046.5)\n",
      "   If (feature 0 <= 2483.5)\n",
      "    If (feature 3 <= 15.0)\n",
      "     If (feature 12 <= 0.5)\n",
      "      If (feature 23 <= 0.5)\n",
      "       Predict: 4.0\n",
      "      Else (feature 23 > 0.5)\n",
      "       Predict: 3.0\n",
      "     Else (feature 12 > 0.5)\n",
      "      Predict: 6.0\n",
      "    Else (feature 3 > 15.0)\n",
      "     If (feature 16 <= 0.5)\n",
      "      Predict: 3.0\n",
      "     Else (feature 16 > 0.5)\n",
      "      If (feature 9 <= 1303.0)\n",
      "       Predict: 3.0\n",
      "      Else (feature 9 > 1303.0)\n",
      "       Predict: 4.0\n",
      "   Else (feature 0 > 2483.5)\n",
      "    If (feature 17 <= 0.5)\n",
      "     If (feature 15 <= 0.5)\n",
      "      Predict: 2.0\n",
      "     Else (feature 15 > 0.5)\n",
      "      Predict: 3.0\n",
      "    Else (feature 17 > 0.5)\n",
      "     If (feature 0 <= 2702.5)\n",
      "      Predict: 3.0\n",
      "     Else (feature 0 > 2702.5)\n",
      "      If (feature 5 <= 1232.0)\n",
      "       Predict: 5.0\n",
      "      Else (feature 5 > 1232.0)\n",
      "       Predict: 2.0\n",
      "  Else (feature 0 > 3046.5)\n",
      "   If (feature 0 <= 3317.5)\n",
      "    If (feature 7 <= 240.5)\n",
      "     Predict: 1.0\n",
      "    Else (feature 7 > 240.5)\n",
      "     If (feature 3 <= 333.0)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 333.0)\n",
      "      If (feature 0 <= 3207.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 0 > 3207.5)\n",
      "       Predict: 1.0\n",
      "   Else (feature 0 > 3317.5)\n",
      "    If (feature 12 <= 0.5)\n",
      "     If (feature 3 <= 296.0)\n",
      "      If (feature 6 <= 207.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 6 > 207.5)\n",
      "       Predict: 7.0\n",
      "     Else (feature 3 > 296.0)\n",
      "      Predict: 1.0\n",
      "    Else (feature 12 > 0.5)\n",
      "     If (feature 45 <= 0.5)\n",
      "      Predict: 7.0\n",
      "     Else (feature 45 > 0.5)\n",
      "      If (feature 5 <= 930.5)\n",
      "       Predict: 7.0\n",
      "      Else (feature 5 > 930.5)\n",
      "       Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70093ad2",
   "metadata": {},
   "source": [
    "Again - the essential configuration for the classifier consists of column names.\n",
    "- the column containing the input feature vectors and the column containing the target value to predict\n",
    "- Because the model will later be used to predict new values of the target, it is given the name of the column to store predictions. \n",
    "- Decision trees are able to assess the importance of input features as part of their building process. That is, they can estimate how much each input feature contributes to making correct predictions. This information is simple to access from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "661d68ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba657735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Elevation</th>\n",
       "      <td>0.827774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_3</th>\n",
       "      <td>0.040477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_1</th>\n",
       "      <td>0.032380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <td>0.027039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <td>0.023549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_31</th>\n",
       "      <td>0.018779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area_2</th>\n",
       "      <td>0.015569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <td>0.004642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_2</th>\n",
       "      <td>0.003780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <td>0.002619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <td>0.002354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_9</th>\n",
       "      <td>0.001037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slope</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area_3</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area_1</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area_0</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_0</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_4</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_5</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspect</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_7</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_6</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_10</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_8</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_12</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_13</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_14</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_11</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_16</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_17</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_18</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_19</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_20</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_21</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_22</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_15</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_23</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_24</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_26</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_25</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_27</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_28</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_29</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_30</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_32</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_33</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_34</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_35</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_36</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_37</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_38</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_39</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    importance\n",
       "Elevation                             0.827774\n",
       "Soil_Type_3                           0.040477\n",
       "Soil_Type_1                           0.032380\n",
       "Hillshade_Noon                        0.027039\n",
       "Horizontal_Distance_To_Hydrology      0.023549\n",
       "Soil_Type_31                          0.018779\n",
       "Wilderness_Area_2                     0.015569\n",
       "Horizontal_Distance_To_Roadways       0.004642\n",
       "Soil_Type_2                           0.003780\n",
       "Hillshade_9am                         0.002619\n",
       "Horizontal_Distance_To_Fire_Points    0.002354\n",
       "Soil_Type_9                           0.001037\n",
       "Vertical_Distance_To_Hydrology        0.000000\n",
       "Slope                                 0.000000\n",
       "Wilderness_Area_3                     0.000000\n",
       "Wilderness_Area_1                     0.000000\n",
       "Wilderness_Area_0                     0.000000\n",
       "Soil_Type_0                           0.000000\n",
       "Soil_Type_4                           0.000000\n",
       "Soil_Type_5                           0.000000\n",
       "Aspect                                0.000000\n",
       "Hillshade_3pm                         0.000000\n",
       "Soil_Type_7                           0.000000\n",
       "Soil_Type_6                           0.000000\n",
       "Soil_Type_10                          0.000000\n",
       "Soil_Type_8                           0.000000\n",
       "Soil_Type_12                          0.000000\n",
       "Soil_Type_13                          0.000000\n",
       "Soil_Type_14                          0.000000\n",
       "Soil_Type_11                          0.000000\n",
       "Soil_Type_16                          0.000000\n",
       "Soil_Type_17                          0.000000\n",
       "Soil_Type_18                          0.000000\n",
       "Soil_Type_19                          0.000000\n",
       "Soil_Type_20                          0.000000\n",
       "Soil_Type_21                          0.000000\n",
       "Soil_Type_22                          0.000000\n",
       "Soil_Type_15                          0.000000\n",
       "Soil_Type_23                          0.000000\n",
       "Soil_Type_24                          0.000000\n",
       "Soil_Type_26                          0.000000\n",
       "Soil_Type_25                          0.000000\n",
       "Soil_Type_27                          0.000000\n",
       "Soil_Type_28                          0.000000\n",
       "Soil_Type_29                          0.000000\n",
       "Soil_Type_30                          0.000000\n",
       "Soil_Type_32                          0.000000\n",
       "Soil_Type_33                          0.000000\n",
       "Soil_Type_34                          0.000000\n",
       "Soil_Type_35                          0.000000\n",
       "Soil_Type_36                          0.000000\n",
       "Soil_Type_37                          0.000000\n",
       "Soil_Type_38                          0.000000\n",
       "Soil_Type_39                          0.000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model.featureImportances.toArray(),\n",
    "            index=input_cols,columns=['importance']).sort_values(by=\"importance\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1878d295",
   "metadata": {},
   "source": [
    "The resulting DecisionTreeClassificationModel is itself a transformer because it can transform a dataframe containing feature vectors into a dataframe also containing predictions. For example, it might be interesting to see what the model predicts on the training data and compare its predictions with the known correct cover type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90aef940",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(assembled_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c656e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------------------------------------------------------------------------------------------------------+\n",
      "|Cover_Type|prediction|probability                                                                                                         |\n",
      "+----------+----------+--------------------------------------------------------------------------------------------------------------------+\n",
      "|6.0       |3.0       |[0.0,0.0,0.052882690511143735,0.6171426664887228,0.021119711730948884,3.3364473508608036E-5,0.30882156679567596,0.0]|\n",
      "|6.0       |4.0       |[0.0,0.0,0.02650429799426934,0.24426934097421205,0.6396848137535817,0.0,0.08954154727793696,0.0]                    |\n",
      "|6.0       |3.0       |[0.0,0.0,0.052882690511143735,0.6171426664887228,0.021119711730948884,3.3364473508608036E-5,0.30882156679567596,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.052882690511143735,0.6171426664887228,0.021119711730948884,3.3364473508608036E-5,0.30882156679567596,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.052882690511143735,0.6171426664887228,0.021119711730948884,3.3364473508608036E-5,0.30882156679567596,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.052882690511143735,0.6171426664887228,0.021119711730948884,3.3364473508608036E-5,0.30882156679567596,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.052882690511143735,0.6171426664887228,0.021119711730948884,3.3364473508608036E-5,0.30882156679567596,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.052882690511143735,0.6171426664887228,0.021119711730948884,3.3364473508608036E-5,0.30882156679567596,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.052882690511143735,0.6171426664887228,0.021119711730948884,3.3364473508608036E-5,0.30882156679567596,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.052882690511143735,0.6171426664887228,0.021119711730948884,3.3364473508608036E-5,0.30882156679567596,0.0]|\n",
      "+----------+----------+--------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"Cover_Type\",\"prediction\",\"probability\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9122584",
   "metadata": {},
   "source": [
    "Based on the above snippet, it looks like the model could use some work. Its predictions look like they are often wrong. The DecisionTreeClassifier implementation has several hyperparameters for which a value must be chosen, and they've all been left to defaults here. Here, the test set can be used to produce an unbiased evaluation of the expected accuracy of a model built with these default hyperparameters. We will now use MulticlassClassificationEvaluator to compute accuracy and other metrics that evaluate the quality of the model's predictions. It is an example of an evaluator in MLlib, which is responsible for assessing the qualuty of an output DataFrame in some way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "672f7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8cbacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Cover_Type\",predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3aa13102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7033842577724764"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.setMetricName(\"accuracy\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14b86b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.688107077334873"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.setMetricName(\"f1\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c622f",
   "metadata": {},
   "source": [
    "Can be useful to also look at the confusion matrix. rows - actual correct columns, columns - predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e544ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = predictions.groupBy(\"Cover_Type\")\\\n",
    "                              .pivot(\"prediction\", range(1,8)).count()\\\n",
    "                              .na.fill(0.0)\\\n",
    "                              .orderBy(\"Cover_Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae99c472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+-----+----+---+---+-----+\n",
      "|Cover_Type|     1|     2|    3|   4|  5|  6|    7|\n",
      "+----------+------+------+-----+----+---+---+-----+\n",
      "|       1.0|131002| 54687|   90|   0| 31|  4| 4829|\n",
      "|       2.0| 53386|197594| 3035|  37|363| 41|  672|\n",
      "|       3.0|     0|  4367|27269| 351| 34| 98|    0|\n",
      "|       4.0|     0|     6| 1313|1158|  0|  0|    0|\n",
      "|       5.0|     0|  7882|  279|   0|420|  0|    0|\n",
      "|       6.0|     0|  4892|10251| 125| 11|445|    0|\n",
      "|       7.0|  8307|    77|    0|   0|  0|  0|10072|\n",
      "+----------+------+------+-----+----+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42282826",
   "metadata": {},
   "source": [
    "Although 70% accuracy sounds decent, its not immediately clear whether it is outstanding or poor. How well would a simplistic approach do to establish a baseline. We could construct a random classifier by picking a class at random in proportion to its prevalence in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6fd0c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "556de34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_probabilities(data):\n",
    "    total = data.count()\n",
    "    return data.groupBy(\"Cover_Type\").count()\\\n",
    "                                     .orderBy(\"Cover_Type\")\\\n",
    "                                     .select(f.col(\"count\").cast(DoubleType()))\\\n",
    "                                     .withColumn(\"count_proportion\", f.col(\"count\")/total)\\\n",
    "                                     .select(\"count_proportion\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5eb24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prior_probabilities = class_probabilities(train_data)\n",
    "test_prior_probabilities = class_probabilities(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7621a3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count_proportion=0.36442897340612623),\n",
       " Row(count_proportion=0.48769708369653314),\n",
       " Row(count_proportion=0.061397975256533774),\n",
       " Row(count_proportion=0.004734978819715251),\n",
       " Row(count_proportion=0.01640325121194048),\n",
       " Row(count_proportion=0.030057653193864598),\n",
       " Row(count_proportion=0.03528008441528651)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prior_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f718bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prior_probabilities = [p[0] for p in train_prior_probabilities]\n",
    "test_prior_probabilities = [p[0] for p in test_prior_probabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d18d876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37706379578644733"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([train_p * cv_p for train_p, cv_p in zip(train_prior_probabilities, test_prior_probabilities)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df2050",
   "metadata": {},
   "source": [
    "So, random guessing achieves 37% accuracy then, which makes the 70% achieved earlier seem like a good result after all. But, the later was achieved with default hyperparameters. We can do better by exploring what the hyperparameters actually mean for the tree-building process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1453517",
   "metadata": {},
   "source": [
    "**Decision Tree Hyperparameters:**\n",
    "- Maximum Depth:\n",
    "Simply limits the number of levels in the decision tree. It is the maximum numver of chained decisions that the classifier will make to classify an example. Useful to limit this to avoid overfitting the training data. The decision tree is responsible for coming up with potential decision rules to try at each level. Decisons are always of the same form for numeric features. decisions are of the form feature >= value, and for categorical features are of the form feature in (value1, value2, ...). So, the set of decision rules to try is really a set of valies to plug in to the decision rule. These are referred to as bins in the PySpark MLlib implementation. A larger number of bins requires more processing time but might lead to finding a more optimal decision rule. What makes a good rule? - Good rules divide the training data's target values into relatively homogenous or pure subsets. Picking a best rule means minimizing the impurity of the 2 subsets it induces. there are 2 commonly used measures of impurity: Gini impurity and entropy.\n",
    "\n",
    "Gini impurity is directly related to the accuracy of the random guess classifier. within a subset - it is the probability that a randomly chosen classification of a random\n",
    "\n",
    "- Finally minimum information gain - hyperparameter that imposes a minimum onformation gain or decrease in impurity for candidate decision rules. Rules that do not improve the subset's impurity enough are rejected. Like a lower maximum depth, this can help the model resist overfitting because decisions that barely help divide the training input may infact not helpfully divide the data at all. **Increasing the minimum information gain hyperparameter increases regularization**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a4b10c",
   "metadata": {},
   "source": [
    "**Tuning Decision Trees:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1265816",
   "metadata": {},
   "source": [
    "Not obvious looking at the data - which impurity measure leads to better accuracy, or what maximum depth, maxBins or minimum information gain hyperparameter is best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc87bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21be06cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"featureVector\")\n",
    "classifier = DecisionTreeClassifier(seed=1234, labelCol=\"Cover_Type\",featuresCol=\"featureVector\",\n",
    "                                    predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "495123d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[assembler,classifier])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66beda4e",
   "metadata": {},
   "source": [
    "Now, we can also define the combinations of hyperparameters that should be tested using the PySpark ML API's built-in support, ParamGridBuilder - plus the evaluation metric that will be used to pock the best hyperparameters, i.e. MulticlassClassificationEvaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c827a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ab6b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder().\\\n",
    "                              addGrid(classifier.impurity, [\"gini\",\"entropy\"])\\\n",
    "                              .addGrid(classifier.maxDepth, [1,20])\\\n",
    "                              .addGrid(classifier.maxBins,[40,300])\\\n",
    "                              .addGrid(classifier.minInfoGain, [0.0, 0.05])\\\n",
    "                              .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f92e500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclassEval = MulticlassClassificationEvaluator()\\\n",
    "                                                    .setLabelCol(\"Cover_Type\")\\\n",
    "                                                    .setPredictionCol(\"prediction\")\\\n",
    "                                                    .setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7f95e",
   "metadata": {},
   "source": [
    "From the param grid - we'll have a total of 16 models evaluated. strategy is the same as with gridsearchcv. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcba926",
   "metadata": {},
   "source": [
    "Finally TrainValidationSplit brings these components together-the pipeline that makes models, model eval metrics, and hyperparameters to try- and can run the eval on training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b14b3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ecf7be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = TrainValidationSplit(seed=1234,\n",
    "                                 estimator=pipeline,\n",
    "                                 evaluator=multiclassEval,\n",
    "                                 estimatorParamMaps=paramGrid,\n",
    "                                 trainRatio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "655203fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_model = validator.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c74614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11cdd515",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = validator_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "838328d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='featuresCol', doc='features column name.'): 'featureVector',\n",
      " Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
      " Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '',\n",
      " Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'entropy',\n",
      " Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256,\n",
      " Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False,\n",
      " Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='labelCol', doc='label column name.'): 'Cover_Type',\n",
      " Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20,\n",
      " Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 300,\n",
      " Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10,\n",
      " Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='seed', doc='random seed.'): 1234,\n",
      " Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
      " Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
      " Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0,\n",
      " Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
      " Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1}\n"
     ]
    }
   ],
   "source": [
    "pprint(best_model.stages[1].extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a297bfb4",
   "metadata": {},
   "source": [
    "May be wondering if it is possible to see the accuracy that each of the models achieved for each combination of hyperparameters. The hyperparameters and evaluations are exposed by the getEstimatorParamMaps and validationMetrics respectively. They can be combined to display all of the parameter combinations sorted by metric value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d057b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = validator_model.validationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9dac76c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = validator_model.getEstimatorParamMaps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d91dbdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_and_params = list(zip(metrics, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59a2dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_and_params.sort(key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a1a5d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9123498354885607,\n",
       "  {Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'entropy',\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 300,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0}),\n",
       " (0.9099012931364298,\n",
       "  {Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'entropy',\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 40,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0}),\n",
       " (0.9059224118142168,\n",
       "  {Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 40,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0}),\n",
       " (0.905214630040554,\n",
       "  {Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 300,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0}),\n",
       " (0.727905731119443,\n",
       "  {Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'entropy',\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 40,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.05}),\n",
       " (0.7266240722319994,\n",
       "  {Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'entropy',\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 300,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.05}),\n",
       " (0.6728900451449996,\n",
       "  {Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 40,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.05}),\n",
       " (0.6720101002372025,\n",
       "  {Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 300,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.05}),\n",
       " (0.6370801132450837,\n",
       "  {Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 1,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 40,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0}),\n",
       " (0.6370801132450837,\n",
       "  {Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 1,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 40,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.05}),\n",
       " (0.636467977657051,\n",
       "  {Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 1,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 300,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0}),\n",
       " (0.636467977657051,\n",
       "  {Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 1,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 300,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.05}),\n",
       " (0.49085622465376083,\n",
       "  {Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'entropy',\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 1,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 40,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0}),\n",
       " (0.49085622465376083,\n",
       "  {Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'entropy',\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 1,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 40,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.05}),\n",
       " (0.49085622465376083,\n",
       "  {Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'entropy',\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 1,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 300,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0}),\n",
       " (0.49085622465376083,\n",
       "  {Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'entropy',\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 1,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 300,\n",
       "   Param(parent='DecisionTreeClassifier_91ca7e7d7de8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.05})]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_and_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "920adda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9114781286711354"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclassEval.evaluate(best_model.transform(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ab3828",
   "metadata": {},
   "source": [
    "**categorical Features Revisited:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ed2172",
   "metadata": {},
   "source": [
    "The categorical features in our dataset are one-hot encoded as several binary 0/1 values. Treating these individual features as numeric turns out to be fine, because any decision rule on numeric features will choose thresholds between 0 and 1 and all are equivalent since all values are 0 or 1.  This encoding forces the decision tree algorithm to consider the values of the underlying categorical features individually. Because features like soil type are broken down into many features and because decision trees treat features individually, its harder to relate information about related soil types. For example, 9 different soil types are actually part of the Leighton family, and they may be related in ways that the decision tree can exploit. If soil type were encoded as a single categorical feature with 40 soil values, then the tree could express rules like if the soil type is one of the 9 leighton family types directly, however when encoded as 40 features, the tree would have to learn a sequence of 9 decsion son soil type to do the same.  \n",
    "\n",
    "Undoing the onehot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ca7403bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unencode_one_hot(data):\n",
    "    wilderness_cols = ['Wilderness_Area_'+str(i) for i in range(4)]\n",
    "    wilderness_assembler = VectorAssembler()\\\n",
    "                                            .setInputCols(wilderness_cols)\\\n",
    "                                            .setOutputCol(\"wilderness\")\n",
    "    unhot_udf = f.udf(lambda v: v.toArray().tolist().index(1))\n",
    "    with_wilderness = wilderness_assembler.transform(data)\\\n",
    "                                          .drop(*wilderness_cols)\\\n",
    "                                          .withColumn(\"wilderness\", unhot_udf(f.col(\"wilderness\")))\n",
    "    soil_cols = ['Soil_Type_'+str(i) for i in range(40)]\n",
    "    soil_assembler = VectorAssembler()\\\n",
    "                                      .setInputCols(soil_cols)\\\n",
    "                                      .setOutputCol(\"soil\")\n",
    "    with_soil = soil_assembler.transform(with_wilderness)\\\n",
    "                              .drop(*soil_cols)\\\n",
    "                              .withColumn(\"soil\", unhot_udf(f.col(\"soil\")))\n",
    "    return with_soil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cef7a5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- Hillshade_9am: integer (nullable = true)\n",
      " |-- Hillshade_Noon: integer (nullable = true)\n",
      " |-- Hillshade_3pm: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Cover_Type: double (nullable = true)\n",
      " |-- wilderness: string (nullable = true)\n",
      " |-- soil: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unenc_train_data = unencode_one_hot(train_data)\n",
    "unenc_train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c8747c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|wilderness| count|\n",
      "+----------+------+\n",
      "|         3| 33315|\n",
      "|         0|234716|\n",
      "|         1| 26840|\n",
      "|         2|228257|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unenc_train_data.groupBy(\"wilderness\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe277a",
   "metadata": {},
   "source": [
    "From here - same process as above van be used to tune the hyperparameters of the decision tree model built on this data and to choose and evaluate the best model. However the two numeric columns have nothing about them that indicates that they're actually an encoding of categorical values. To treat them as numbers is not correct, as their order is meaningless. Internally, MLlib can store additional metadata about each column. The details of this data are generally hidden from the caller but include information such as whether the column encodes a categorical value and how many distinct values it takes on. To add this metadata, its necessary to put the data through VectorIndexer. Its job is to turn input into properly labelled categorical feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "07fa5ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a1231140",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = unenc_train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d577c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCols = [c for c in cols if c!='Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ad2d666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler().setInputCols(inputCols).setOutputCol(\"featureVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "84200281",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = VectorIndexer()\\\n",
    "                        .setMaxCategories(40)\\\n",
    "                        .setInputCol(\"featureVector\").setOutputCol(\"indexedVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9117948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier().setLabelCol(\"Cover_Type\")\\\n",
    "                                     .setFeaturesCol(\"indexedVector\")\\\n",
    "                                     .setPredictionCol(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1d3f0de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages([assembler, indexer, classifier])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b41158",
   "metadata": {},
   "source": [
    ".....train and eval metrics\n",
    "..... hyperparameter search//"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e86c7",
   "metadata": {},
   "source": [
    "**Random Forests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a02dafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e449dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(seed=1234, labelCol=\"Cover_Type\",\n",
    "                                    featuresCol=\"indexedVector\",predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdfc81f",
   "metadata": {},
   "source": [
    "Note - this classifier has another hyperparameter: the number of trees to build. Like the max bins hyperparameter, higher values should give better results up to a point. The cost, however, is that building many trees of course takes many times longer than building one. \n",
    "The accuracy of the best random forest model produced from a similar tuning process is 95% off the bat - about 2% better already"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d557a9",
   "metadata": {},
   "source": [
    "...apply same steps as with trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e74c31",
   "metadata": {},
   "source": [
    "Random forests are appealing in the context of big data becaue trees are supposed to be built independently, and big data technologies like spark and mapreduce inherently need data-parallel problems - where parts of the overall solution can be computed independently on parts of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d2d67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa320605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
