{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684778d5",
   "metadata": {},
   "source": [
    "**LSTM IN PRACTICE:**\n",
    "- Fine grained operation of the LSTM, including layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca81c359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x133139fb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd847a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layer = nn.LSTM(input_size=5, hidden_size=2, num_layers=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f299f3",
   "metadata": {},
   "source": [
    "Inspecting the LSTM's variables - weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b555091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wi = lstm_layer.weight_ih_l0 # translates to these weights: (W_ii|W_if|W_ig|W_io)\n",
    "wh = lstm_layer.weight_hh_l0 # translates to these weights: (W_hi|W_hf|W_hg|W_ho)\n",
    "bi = lstm_layer.bias_ih_l0 # translates to these biases: (b_ii|b_if|b_ig|b_io)\n",
    "bh = lstm_layer.bias_hh_l0 # translates to these biases: (b_hi|b_hf|b_hg|b_ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22ed3642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 5]), torch.Size([8, 2]), torch.Size([8]), torch.Size([8]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi.shape, wh.shape, bi.shape, bh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde07c37",
   "metadata": {},
   "source": [
    "Weights and biases make perfect sense, considering the defined layer shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a41104a",
   "metadata": {},
   "source": [
    "Inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86ff6b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_seq = torch.tensor([[1.0]*5, [2.0]*5, [3.0]*5]).unsqueeze(0).float()\n",
    "x_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6387907e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 2]) torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "output, (hn, cn) = lstm_layer(x_seq)\n",
    "print(output.shape, hn.shape, cn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43da90",
   "metadata": {},
   "source": [
    "Working the lstm output out manually:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018720d7",
   "metadata": {},
   "source": [
    "First, lets break down the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "140ed47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wii, wif, wig, wio = wi[:2,:], wi[2:4,:], wi[4:6,:], wi[6:,:]\n",
    "whi, whf, whg, who = wh[:2,:], wh[2:4,:], wh[4:6,:], wh[6:,:]\n",
    "bii, bif, big, bio = bi[:2], bi[2:4], bi[4:6], bi[6:]\n",
    "bhi, bhf, bhg, bho = bh[:2], bh[2:4], bh[4:6], bh[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12762b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_state = []\n",
    "short_state = []\n",
    "\n",
    "# loop over the sequence length: unroll through time\n",
    "for t in range(3):\n",
    "    # for each timestep, get the input at that corresponds to that timestep\n",
    "    xt = x_seq[:, t, :]\n",
    "\n",
    "    # making all computations that don't need any form of hidden state\n",
    "    # first part of the input gate:\n",
    "    it1 = torch.matmul(xt, torch.transpose(wii,0,1)) + bii\n",
    "    # first part of the forget gate:\n",
    "    ft1 = torch.matmul(xt, torch.transpose(wif,0,1)) + bif\n",
    "    # first part of the new signal processed: g\n",
    "    gt1 = torch.matmul(xt, torch.transpose(wig,0,1)) + big\n",
    "    # first part of the output gate\n",
    "    ot1 = torch.matmul(xt, torch.transpose(wio,0,1)) + bio\n",
    "\n",
    "    if t>0:\n",
    "        prev_h = short_state[t-1]\n",
    "        prev_c = long_state[t-1]\n",
    "    else:\n",
    "        prev_h = torch.zeros((xt.shape[0], 2)) # at timestep 0, we use zeros of the shape we're trying to project our inputs to\n",
    "        prev_c = torch.zeros((xt.shape[0], 2))\n",
    "    # finish up the input gate:\n",
    "    #it2 = torch.matmul(prev_h, whi) + bhi\n",
    "    it = torch.sigmoid(it1+ torch.matmul(prev_h, torch.transpose(whi,0,1)) + bhi)\n",
    "\n",
    "    # finish up the forget gate:\n",
    "    #ft2 = torch.matmul(prev_h, whf) + bhf\n",
    "    ft = torch.sigmoid(ft1 + torch.matmul(prev_h, torch.transpose(whf,0,1)) + bhf)\n",
    "\n",
    "    # finish up the candidate signal\n",
    "    #gt2 = torch.matmul(prev_h,whg) + bhg\n",
    "    gt = torch.tanh(gt1+torch.matmul(prev_h, torch.transpose(whg,0,1)) + bhg)\n",
    "\n",
    "    # finish up the output gate:\n",
    "    #ot2 = torch.matmul(prev_h, who) + bho\n",
    "    ot = torch.sigmoid(ot1+torch.matmul(prev_h, torch.transpose(who,0,1)) + bho)\n",
    "\n",
    "\n",
    "    long_state_t = (ft*prev_c) + (it*gt)\n",
    "    short_state_t = ot*(torch.tanh(long_state_t))\n",
    "\n",
    "    long_state.append(long_state_t)\n",
    "    short_state.append(short_state_t)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e305788c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2660,  0.6454]]], grad_fn=<StackBackward0>),\n",
       " tensor([[[-0.4218,  1.4677]]], grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn,cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1841c3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2660,  0.6454]], grad_fn=<MulBackward0>),\n",
       " tensor([[-0.4218,  1.4677]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_state[-1], long_state[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dea4b05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1993,  0.1615],\n",
       "         [-0.2559,  0.4308],\n",
       "         [-0.2660,  0.6454]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf2c23",
   "metadata": {},
   "source": [
    "Correctly implemented a very crude version of the lstm cell above:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d52f742",
   "metadata": {},
   "source": [
    "That code is too crude however, what if we had an lstm cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "086edd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell = nn.LSTMCell(5,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113147f",
   "metadata": {},
   "source": [
    "Initialize the weights to the lstm layer's weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb4f5cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell.weight_ih.data = wi\n",
    "lstm_cell.weight_hh.data = wh\n",
    "lstm_cell.bias_ih.data = bi\n",
    "lstm_cell.bias_hh.data = bh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faa86fd",
   "metadata": {},
   "source": [
    "compute the output for the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4235932",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_state = []\n",
    "short_state = []\n",
    "\n",
    "for t in range(3):\n",
    "    xt = x_seq[:,t,:]\n",
    "    if t > 0:\n",
    "        prev_h = short_state[t-1]\n",
    "        prev_c = long_state[t-1]\n",
    "    else:\n",
    "        prev_h = torch.zeros((xt.shape[0],2))\n",
    "        prev_c = torch.zeros((xt.shape[0],2))\n",
    "    \n",
    "    short_state_t, long_state_t = lstm_cell(xt, (prev_h, prev_c))\n",
    "\n",
    "    long_state.append(long_state_t)\n",
    "    short_state.append(short_state_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "695700bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2660,  0.6454]]], grad_fn=<StackBackward0>),\n",
       " tensor([[[-0.4218,  1.4677]]], grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn,cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8408a6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2660,  0.6454]], grad_fn=<MulBackward0>),\n",
       " tensor([[-0.4218,  1.4677]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_state[-1],long_state[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d51e0d",
   "metadata": {},
   "source": [
    "So, clearly using an lstm cell alone is more elegant and clean code. If we were to use layer normalization and an lstm cell, there would be no way of having the layer normalization in before the final activation functions - hence the need for a custom cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9010574",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(input_size, 4*hidden_size) # instead of breaking this down, could just compute these with one linear layer and break down its results later\n",
    "        self.hidden = nn.Linear(hidden_size, 4*hidden_size)n # same for this\n",
    "    def forward(self, x, prev_h, prev_c):\n",
    "        computed_inputs = self.input(x)\n",
    "        computed_hiddens = self.hidden(prev_h)\n",
    "        # compute the input gate\n",
    "        input_gate = torch.sigmoid(computed_inputs[:,:2]+computed_hiddens[:,:2])\n",
    "        forget_gate = torch.sigmoid(computed_inputs[:,2:4]+computed_hiddens[:,2:4])\n",
    "        signal = torch.tanh(computed_inputs[:,4:6]+computed_hiddens[:,4:6])\n",
    "        output_gate = torch.sigmoid(computed_inputs[:,6:]+computed_hiddens[:,6:])\n",
    "        long_state = forget_gate*prev_c + input_gate*signal\n",
    "        return output_gate*torch.tanh(long_state), long_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3eddba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_lstm = CustomLSTMCell(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4e58b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_lstm.input.weight.data = wi\n",
    "custom_lstm.input.bias.data = bi\n",
    "custom_lstm.hidden.weight.data = wh\n",
    "custom_lstm.hidden.bias.data=bh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4309351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_state = []\n",
    "short_state = []\n",
    "\n",
    "for t in range(3):\n",
    "    xt = x_seq[:,t,:]\n",
    "    if t > 0:\n",
    "        prev_h = short_state[t-1]\n",
    "        prev_c = long_state[t-1]\n",
    "    else:\n",
    "        prev_h = torch.zeros((xt.shape[0],2))\n",
    "        prev_c = torch.zeros((xt.shape[0],2))\n",
    "    \n",
    "    short_state_t, long_state_t = custom_lstm(xt, prev_h, prev_c)\n",
    "\n",
    "    long_state.append(long_state_t)\n",
    "    short_state.append(short_state_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4828cae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2660,  0.6454]]], grad_fn=<StackBackward0>),\n",
       " tensor([[[-0.4218,  1.4677]]], grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn,cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb34c8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2660,  0.6454]], grad_fn=<MulBackward0>),\n",
       " tensor([[-0.4218,  1.4677]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_state[-1],long_state[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624aa3a2",
   "metadata": {},
   "source": [
    "Works fine, now a custom lstm cell with layer normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edff1c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(input_size, 4*hidden_size) # instead of breaking this down, could just compute these with one linear layer and break down its results later\n",
    "        self.hidden = nn.Linear(hidden_size, 4*hidden_size) # same for this\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "    def forward(self, x, prev_h, prev_c):\n",
    "        computed_inputs = self.input(x)\n",
    "        computed_hiddens = self.hidden(prev_h)\n",
    "        # compute the input gate\n",
    "        input_gate = torch.sigmoid(computed_inputs[:,:self.hidden_size]+computed_hiddens[:,:self.hidden_size])\n",
    "        forget_gate = torch.sigmoid(computed_inputs[:,self.hidden_size:2*self.hidden_size]+computed_hiddens[:,self.hidden_size:2*self.hidden_size])\n",
    "        signal = torch.tanh(computed_inputs[:,2*self.hidden_size:3*self.hidden_size]+computed_hiddens[:,2*self.hidden_size:3*self.hidden_size])\n",
    "        output_gate = torch.sigmoid(computed_inputs[:,3*self.hidden_size:4*self.hidden_size]+computed_hiddens[:,3*self.hidden_size:4*self.hidden_size])\n",
    "        long_state = forget_gate*prev_c + input_gate*signal\n",
    "        return output_gate*torch.tanh(self.ln(long_state)), long_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd06c015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
