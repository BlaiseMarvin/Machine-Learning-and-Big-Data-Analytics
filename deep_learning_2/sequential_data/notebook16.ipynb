{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9fd5f5",
   "metadata": {},
   "source": [
    "**WaveNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9070aa1",
   "metadata": {},
   "source": [
    "Implementing a simple version of the wavenet model whic uses convlayers with the dilation doubled at every layer. In return the general idea behind this model is that the lower layers will learn short time sequences and the higher layers long term sequences. Thats basically the general idea behind the wavenet model. Stack convnets together while doubling dilation -> doubling the dilation ensures that we have a bigger receptive field without increasing the number of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6015673",
   "metadata": {},
   "source": [
    "keras implementation:\n",
    "wavenet_model = tf.keras.Sequential()\n",
    "wavenet_model.add(tf.keras.layers.Input(shape=[None, 5]))\n",
    "for rate in (1, 2, 4, 8) * 2:\n",
    "wavenet_model.add(tf.keras.layers.Conv1D(\n",
    "filters=32, kernel_size=2, padding=\"causal\", activation=\"relu\",\n",
    "dilation_rate=rate))\n",
    "wavenet_model.add(tf.keras.layers.Conv1D(filters=14, kernel_size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b1bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "170531cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size # the conv layer's kernel size\n",
    "        self.dilation = dilation # the dilation\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            dilation=dilation,\n",
    "            padding=0,\n",
    "            **kwargs\n",
    "        ) # the conv layer we ough to manipulate and introduce left padding to\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Calculate the total padding needed\n",
    "        # Essentially, we are using a stride of 1 -> easy to compute how much we are taking away from the \n",
    "        # input sequence in terms of length and then add this deficit to the start of the signal as padded zeros\n",
    "        # compute the total padding needed\n",
    "        pad_left = (self.kernel_size - 1)*self.dilation\n",
    "        # pad left only (causal) padding\n",
    "        x = F.pad(x, (pad_left, 0)) # applying padding but only to the left as size is only specified to the left and zero padding for the right. default for this func is constant padding of zero which is what is being applied\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cdbc6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetStack(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        dilations = [1,2,4,8]*2\n",
    "\n",
    "        in_channels = 1\n",
    "        layers = []\n",
    "\n",
    "        for d in dilations:\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    CausalConv1d(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=32,\n",
    "                        kernel_size=2,\n",
    "                        dilation=d\n",
    "                    ),\n",
    "                    nn.BatchNorm1d(32),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "            in_channels=32\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.out_layer = nn.Conv1d(in_channels=32,out_channels=14,kernel_size=1)\n",
    "        # applying a 1d conv layer with kernel size of 1 -> which essentially functions like a dense/linear layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.out_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6c8582",
   "metadata": {},
   "source": [
    "Get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "636e528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utils.fetch_data import fetch_timeseries_data\n",
    "from utils.early_stopping import EarlyStopping\n",
    "from utils.fetch_data import create_splits\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01eb7ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x109ab6070>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21105c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_type</th>\n",
       "      <th>bus</th>\n",
       "      <th>rail</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001-01-01</th>\n",
       "      <td>U</td>\n",
       "      <td>297192</td>\n",
       "      <td>126455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-02</th>\n",
       "      <td>W</td>\n",
       "      <td>780827</td>\n",
       "      <td>501952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-03</th>\n",
       "      <td>W</td>\n",
       "      <td>824923</td>\n",
       "      <td>536432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-04</th>\n",
       "      <td>W</td>\n",
       "      <td>870021</td>\n",
       "      <td>550011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-05</th>\n",
       "      <td>W</td>\n",
       "      <td>890426</td>\n",
       "      <td>557917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           day_type     bus    rail\n",
       "date                               \n",
       "2001-01-01        U  297192  126455\n",
       "2001-01-02        W  780827  501952\n",
       "2001-01-03        W  824923  536432\n",
       "2001-01-04        W  870021  550011\n",
       "2001-01-05        W  890426  557917"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = fetch_timeseries_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69c37701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9009, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "810b7264",
   "metadata": {},
   "outputs": [],
   "source": [
    "rail_train, rail_valid, rail_test = create_splits(df, attr='rail', train_ran=['2014-01','2023-12'],val_ran=['2024-01','2024-12'],test_ran=['2025-01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2940817",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 56\n",
    "forecasting_horizon = 14\n",
    "\n",
    "def create_x_chunks(ds):\n",
    "    return [ds[i:i+seq_length] for i in range(len(ds)-seq_length-forecasting_horizon+1)]\n",
    "\n",
    "def create_y_chunks(ds):\n",
    "    y_chunks = []\n",
    "    for i in range(len(ds)-seq_length-forecasting_horizon+1):\n",
    "        y_chunks.append([ds[i+1+j:i+1+j+forecasting_horizon] for j in range(seq_length)])\n",
    "    return y_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d7d7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_chunks, train_y_chunks = create_x_chunks(rail_train.values.tolist()), create_y_chunks(rail_train.values.tolist())\n",
    "valid_x_chunks, valid_y_chunks = create_x_chunks(rail_valid.values.tolist()), create_y_chunks(rail_valid.values.tolist())\n",
    "test_x_chunks, test_y_chunks = create_x_chunks(rail_test.values.tolist()), create_y_chunks(rail_test.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2f2f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_chunks, train_y_chunks = torch.tensor(train_x_chunks), torch.tensor(train_y_chunks)\n",
    "valid_x_chunks, valid_y_chunks = torch.tensor(valid_x_chunks), torch.tensor(valid_y_chunks)\n",
    "test_x_chunks, test_y_chunks = torch.tensor(test_x_chunks), torch.tensor(test_y_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c02f1fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3583, 56]), torch.Size([3583, 56, 14]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_chunks.shape, train_y_chunks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5809ac",
   "metadata": {},
   "source": [
    "Build the dataset using chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aee76266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsDataset(Dataset):\n",
    "    def __init__(self, x_chunks, y_chunks):\n",
    "        super().__init__()\n",
    "        self.x_chunks = x_chunks.unsqueeze(2)\n",
    "        self.y_chunks = y_chunks\n",
    "    def __len__(self):\n",
    "        return self.x_chunks.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_chunks[index,:,:],self.y_chunks[index,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2969cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TsDataset(train_x_chunks, train_y_chunks)\n",
    "valid_ds = TsDataset(valid_x_chunks, valid_y_chunks)\n",
    "test_ds = TsDataset(test_x_chunks, test_y_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfc1cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=128)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=128)\n",
    "test_dl = DataLoader(test_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021940f0",
   "metadata": {},
   "source": [
    "Model and training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b0d36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d3be280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x109ab6070>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "11be0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaveNetStack().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "adafbeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9de46e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.002)\n",
    "criterion = nn.HuberLoss(reduction=\"sum\")\n",
    "early_stopper = EarlyStopping(patience=50, checkpoint_path='wavenet.pt', restore_best_weights=True, verbose=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0f1d0e",
   "metadata": {},
   "source": [
    "Training Loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e80d0306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1| Train loss: 41.7934| Val loss: 10.5909\n",
      "Metric improved to 10.5909. Checkpoint saved at epoch 0\n",
      "Epoch: 2| Train loss: 6.0512| Val loss: 4.5821\n",
      "Metric improved to 4.5821. Checkpoint saved at epoch 1\n",
      "Epoch: 3| Train loss: 4.1449| Val loss: 3.0476\n",
      "Metric improved to 3.0476. Checkpoint saved at epoch 2\n",
      "Epoch: 4| Train loss: 3.5616| Val loss: 2.3631\n",
      "Metric improved to 2.3631. Checkpoint saved at epoch 3\n",
      "Epoch: 5| Train loss: 3.1561| Val loss: 2.1539\n",
      "Metric improved to 2.1539. Checkpoint saved at epoch 4\n",
      "Epoch: 6| Train loss: 3.0345| Val loss: 2.1233\n",
      "Metric improved to 2.1233. Checkpoint saved at epoch 5\n",
      "Epoch: 7| Train loss: 2.7860| Val loss: 1.9633\n",
      "Metric improved to 1.9633. Checkpoint saved at epoch 6\n",
      "Epoch: 8| Train loss: 2.6475| Val loss: 1.7952\n",
      "Metric improved to 1.7952. Checkpoint saved at epoch 7\n",
      "Epoch: 9| Train loss: 2.5493| Val loss: 1.7672\n",
      "Metric improved to 1.7672. Checkpoint saved at epoch 8\n",
      "Epoch: 10| Train loss: 2.5360| Val loss: 1.6556\n",
      "Metric improved to 1.6556. Checkpoint saved at epoch 9\n",
      "Epoch: 11| Train loss: 2.5189| Val loss: 1.7520\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch: 12| Train loss: 2.4877| Val loss: 1.6290\n",
      "Metric improved to 1.6290. Checkpoint saved at epoch 11\n",
      "Epoch: 13| Train loss: 2.3615| Val loss: 1.4377\n",
      "Metric improved to 1.4377. Checkpoint saved at epoch 12\n",
      "Epoch: 14| Train loss: 2.2985| Val loss: 1.4358\n",
      "Metric improved to 1.4358. Checkpoint saved at epoch 13\n",
      "Epoch: 15| Train loss: 2.2529| Val loss: 1.6015\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch: 16| Train loss: 2.3059| Val loss: 1.3720\n",
      "Metric improved to 1.3720. Checkpoint saved at epoch 15\n",
      "Epoch: 17| Train loss: 2.2733| Val loss: 1.5167\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch: 18| Train loss: 2.2404| Val loss: 1.5986\n",
      "No improvement for 2 epoch(s)\n",
      "Epoch: 19| Train loss: 2.1416| Val loss: 1.5658\n",
      "No improvement for 3 epoch(s)\n",
      "Epoch: 20| Train loss: 2.1515| Val loss: 1.5791\n",
      "No improvement for 4 epoch(s)\n",
      "Epoch: 21| Train loss: 2.1129| Val loss: 1.4008\n",
      "No improvement for 5 epoch(s)\n",
      "Epoch: 22| Train loss: 2.1175| Val loss: 1.3490\n",
      "Metric improved to 1.3490. Checkpoint saved at epoch 21\n",
      "Epoch: 23| Train loss: 2.0558| Val loss: 1.3690\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch: 24| Train loss: 2.0968| Val loss: 1.3962\n",
      "No improvement for 2 epoch(s)\n",
      "Epoch: 25| Train loss: 2.0017| Val loss: 1.4645\n",
      "No improvement for 3 epoch(s)\n",
      "Epoch: 26| Train loss: 2.0198| Val loss: 1.4019\n",
      "No improvement for 4 epoch(s)\n",
      "Epoch: 27| Train loss: 2.0058| Val loss: 1.4457\n",
      "No improvement for 5 epoch(s)\n",
      "Epoch: 28| Train loss: 1.9462| Val loss: 1.2995\n",
      "Metric improved to 1.2995. Checkpoint saved at epoch 27\n",
      "Epoch: 29| Train loss: 1.9422| Val loss: 1.4495\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch: 30| Train loss: 1.9462| Val loss: 1.3201\n",
      "No improvement for 2 epoch(s)\n",
      "Epoch: 31| Train loss: 1.8797| Val loss: 1.3543\n",
      "No improvement for 3 epoch(s)\n",
      "Epoch: 32| Train loss: 1.9178| Val loss: 1.3123\n",
      "No improvement for 4 epoch(s)\n",
      "Epoch: 33| Train loss: 1.9021| Val loss: 1.4052\n",
      "No improvement for 5 epoch(s)\n",
      "Epoch: 34| Train loss: 1.8890| Val loss: 1.3920\n",
      "No improvement for 6 epoch(s)\n",
      "Epoch: 35| Train loss: 1.8223| Val loss: 1.3306\n",
      "No improvement for 7 epoch(s)\n",
      "Epoch: 36| Train loss: 1.7932| Val loss: 1.3702\n",
      "No improvement for 8 epoch(s)\n",
      "Epoch: 37| Train loss: 1.7757| Val loss: 1.2948\n",
      "Metric improved to 1.2948. Checkpoint saved at epoch 36\n",
      "Epoch: 38| Train loss: 1.7803| Val loss: 1.2598\n",
      "Metric improved to 1.2598. Checkpoint saved at epoch 37\n",
      "Epoch: 39| Train loss: 1.7680| Val loss: 1.3294\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch: 40| Train loss: 1.7562| Val loss: 1.3512\n",
      "No improvement for 2 epoch(s)\n",
      "Epoch: 41| Train loss: 1.7506| Val loss: 1.3328\n",
      "No improvement for 3 epoch(s)\n",
      "Epoch: 42| Train loss: 1.7506| Val loss: 1.3074\n",
      "No improvement for 4 epoch(s)\n",
      "Epoch: 43| Train loss: 1.7963| Val loss: 1.3325\n",
      "No improvement for 5 epoch(s)\n",
      "Epoch: 44| Train loss: 1.7575| Val loss: 1.3181\n",
      "No improvement for 6 epoch(s)\n",
      "Epoch: 45| Train loss: 1.7659| Val loss: 1.3710\n",
      "No improvement for 7 epoch(s)\n",
      "Epoch: 46| Train loss: 1.7434| Val loss: 1.3490\n",
      "No improvement for 8 epoch(s)\n",
      "Epoch: 47| Train loss: 1.7756| Val loss: 1.3107\n",
      "No improvement for 9 epoch(s)\n",
      "Epoch: 48| Train loss: 1.7701| Val loss: 1.3137\n",
      "No improvement for 10 epoch(s)\n",
      "Epoch: 49| Train loss: 1.7408| Val loss: 1.3186\n",
      "No improvement for 11 epoch(s)\n",
      "Epoch: 50| Train loss: 1.7671| Val loss: 1.3372\n",
      "No improvement for 12 epoch(s)\n",
      "Epoch: 51| Train loss: 1.7318| Val loss: 1.3416\n",
      "No improvement for 13 epoch(s)\n",
      "Epoch: 52| Train loss: 1.7313| Val loss: 1.3174\n",
      "No improvement for 14 epoch(s)\n",
      "Epoch: 53| Train loss: 1.7437| Val loss: 1.3387\n",
      "No improvement for 15 epoch(s)\n",
      "Epoch: 54| Train loss: 1.7342| Val loss: 1.3301\n",
      "No improvement for 16 epoch(s)\n",
      "Epoch: 55| Train loss: 1.7431| Val loss: 1.2987\n",
      "No improvement for 17 epoch(s)\n",
      "Epoch: 56| Train loss: 1.7679| Val loss: 1.3109\n",
      "No improvement for 18 epoch(s)\n",
      "Epoch: 57| Train loss: 1.7241| Val loss: 1.3507\n",
      "No improvement for 19 epoch(s)\n",
      "Epoch: 58| Train loss: 1.7168| Val loss: 1.3278\n",
      "No improvement for 20 epoch(s)\n",
      "Epoch: 59| Train loss: 1.7339| Val loss: 1.3354\n",
      "No improvement for 21 epoch(s)\n",
      "Epoch: 60| Train loss: 1.7374| Val loss: 1.3414\n",
      "No improvement for 22 epoch(s)\n",
      "Epoch: 61| Train loss: 1.7281| Val loss: 1.3338\n",
      "No improvement for 23 epoch(s)\n",
      "Epoch: 62| Train loss: 1.7421| Val loss: 1.3299\n",
      "No improvement for 24 epoch(s)\n",
      "Epoch: 63| Train loss: 1.7306| Val loss: 1.3397\n",
      "No improvement for 25 epoch(s)\n",
      "Epoch: 64| Train loss: 1.7559| Val loss: 1.3509\n",
      "No improvement for 26 epoch(s)\n",
      "Epoch: 65| Train loss: 1.7401| Val loss: 1.3256\n",
      "No improvement for 27 epoch(s)\n",
      "Epoch: 66| Train loss: 1.7365| Val loss: 1.3292\n",
      "No improvement for 28 epoch(s)\n",
      "Epoch: 67| Train loss: 1.7287| Val loss: 1.3293\n",
      "No improvement for 29 epoch(s)\n",
      "Epoch: 68| Train loss: 1.7363| Val loss: 1.3191\n",
      "No improvement for 30 epoch(s)\n",
      "Epoch: 69| Train loss: 1.7445| Val loss: 1.3211\n",
      "No improvement for 31 epoch(s)\n",
      "Epoch: 70| Train loss: 1.7236| Val loss: 1.3266\n",
      "No improvement for 32 epoch(s)\n",
      "Epoch: 71| Train loss: 1.7268| Val loss: 1.3320\n",
      "No improvement for 33 epoch(s)\n",
      "Epoch: 72| Train loss: 1.7465| Val loss: 1.3204\n",
      "No improvement for 34 epoch(s)\n",
      "Epoch: 73| Train loss: 1.7278| Val loss: 1.3450\n",
      "No improvement for 35 epoch(s)\n",
      "Epoch: 74| Train loss: 1.7303| Val loss: 1.3317\n",
      "No improvement for 36 epoch(s)\n",
      "Epoch: 75| Train loss: 1.7352| Val loss: 1.3197\n",
      "No improvement for 37 epoch(s)\n",
      "Epoch: 76| Train loss: 1.7117| Val loss: 1.3252\n",
      "No improvement for 38 epoch(s)\n",
      "Epoch: 77| Train loss: 1.7300| Val loss: 1.3230\n",
      "No improvement for 39 epoch(s)\n",
      "Epoch: 78| Train loss: 1.7270| Val loss: 1.3422\n",
      "No improvement for 40 epoch(s)\n",
      "Epoch: 79| Train loss: 1.7443| Val loss: 1.3473\n",
      "No improvement for 41 epoch(s)\n",
      "Epoch: 80| Train loss: 1.7215| Val loss: 1.3301\n",
      "No improvement for 42 epoch(s)\n",
      "Epoch: 81| Train loss: 1.7293| Val loss: 1.3255\n",
      "No improvement for 43 epoch(s)\n",
      "Epoch: 82| Train loss: 1.7645| Val loss: 1.3285\n",
      "No improvement for 44 epoch(s)\n",
      "Epoch: 83| Train loss: 1.7200| Val loss: 1.3232\n",
      "No improvement for 45 epoch(s)\n",
      "Epoch: 84| Train loss: 1.7495| Val loss: 1.3294\n",
      "No improvement for 46 epoch(s)\n",
      "Epoch: 85| Train loss: 1.7221| Val loss: 1.3207\n",
      "No improvement for 47 epoch(s)\n",
      "Epoch: 86| Train loss: 1.7287| Val loss: 1.3304\n",
      "No improvement for 48 epoch(s)\n",
      "Epoch: 87| Train loss: 1.7451| Val loss: 1.3210\n",
      "No improvement for 49 epoch(s)\n",
      "Epoch: 88| Train loss: 1.7245| Val loss: 1.3273\n",
      "No improvement for 50 epoch(s)\n",
      "Early stopping triggered after 50 epochs of no improvement\n",
      "Restored best model and optimizer states from checkpoint\n",
      "Stopping at epoch: 88\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "\n",
    "train_loss = [0]*n_epochs\n",
    "val_loss = [0]*n_epochs\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    # iterate through the training data\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        x_batch = x_batch.permute(0,2,1)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x_batch)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        # adding l1 norm\n",
    "        norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss = criterion(out, y_batch)\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        train_loss[epoch]+=loss.item()\n",
    "    train_loss[epoch] /= len(train_dl.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x_batch,y_batch in valid_dl:\n",
    "            x_batch,y_batch = x_batch.to(device),y_batch.to(device)\n",
    "            x_batch = x_batch.permute(0,2,1)\n",
    "            out = model(x_batch)\n",
    "            out = out.permute(0,2,1)\n",
    "            loss = criterion(out, y_batch)\n",
    "            val_loss[epoch] += loss.item()\n",
    "        val_loss[epoch] /= len(valid_dl.dataset)\n",
    "\n",
    "        scheduler.step(val_loss[epoch])\n",
    "        print(f'Epoch: {epoch+1}| Train loss: {train_loss[epoch]:.4f}| Val loss: {val_loss[epoch]:.4f}')\n",
    "        early_stopper(val_loss[epoch], model, optimizer, epoch)\n",
    "        if early_stopper.should_stop:\n",
    "            print(f\"Stopping at epoch: {epoch+1}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e7a8e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = nn.L1Loss(reduction='mean')\n",
    "loss_ = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for x_batch, y_batch in test_dl:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        x_batch = x_batch.permute(0,2,1)\n",
    "        out = model(x_batch)\n",
    "        out = out.permute(0,2,1)\n",
    "        loss = l1_loss(out, y_batch)\n",
    "        loss_.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7996c606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44534.552842378616"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(loss_)/len(loss_)) * 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca2392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
