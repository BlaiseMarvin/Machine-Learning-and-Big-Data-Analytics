{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f8f0d6c",
   "metadata": {},
   "source": [
    "**Project two - character level language modelling in PyTorch:**\n",
    "- Language modelling is a fascinating application that enables machines to perform human language-related tasks, such as generating English sentences. In the model we build, the inpput is a text document, and the goal is to develop a model that can generate new text that is similar in style to the input document. Examples of such input are a book, or a computer program in a specific programming language\n",
    "- In character-level language modelling, the input is broken down to a sequence of characters that are fed into the network one character at a time. The network then then processes each new character in conjunction with the memory of the previously seen characters to predict the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d10155",
   "metadata": {},
   "source": [
    "**Preprocessing the dataset:**\n",
    "- Data used: book: The Mysterious Island by Jules Verne. available on gutenberg.org as file 1268-0.txt\n",
    "- Once downloaded, we can now read the dataset into a python session as plain text - using the following code, we will read text directly from the downloaded file and remove portions from the beginning and end (these contain certain descriptions of the Gutenberg project). Then, we will create a python variable char_set, that represents the set of unique characters observed in this text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75e3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b96da131",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/blaise/Documents/ML/Machine-Learning-and-Big-Data-Analytics/data/1268-0.txt', 'r', encoding='utf-8') as fp:\n",
    "    text = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bca1bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_indx = text.find('End of the Project Gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0281bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[start_indx:end_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff6bfbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a97676a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length:  1112310\n",
      "Unique characters:  80\n"
     ]
    }
   ],
   "source": [
    "print('Total Length: ', len(text))\n",
    "print('Unique characters: ', len(char_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8af78bc",
   "metadata": {},
   "source": [
    "We need to map the characters to integers. To do this, we create a simple python dictionary that maps each character to an integer, char2int. We also need a reverse mapping to convert the results of our model back to text. Although the reverse can be done using a dictionary that associates integer keys with character values, using a Numpy Array and indexing the array ton map indices to those unique characters is more efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9a6017",
   "metadata": {},
   "source": [
    "Building a dictionary to map characters to integers, and reverse mapping via indexing a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "548aa1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
    "char_array = np.array(chars_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8c9410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoded = np.array(\n",
    "    [char2int[ch] for ch in text],\n",
    "    dtype=np.int32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d4c16ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44, 32, 29, ...,  6,  6,  6], shape=(1112310,), dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26988097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape:  (1112310,)\n"
     ]
    }
   ],
   "source": [
    "print('Text encoded shape: ', text_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3bcebae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE MYSTERIOUS   == Encoding ==>  [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n"
     ]
    }
   ],
   "source": [
    "print(text[:15], \" == Encoding ==> \", text_encoded[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d06315b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]  == Reverse ==>  THE MYSTERIOUS \n"
     ]
    }
   ],
   "source": [
    "print(text_encoded[:15], \" == Reverse ==> \",''.join(char_array[text_encoded[:15]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa5e3a",
   "metadata": {},
   "source": [
    "So, the text_encoded numpy array contains the encoded values for all characters in the text. Now, we will print out the mappings of the first 5 characters from this array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f22fa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 -> T\n",
      "32 -> H\n",
      "29 -> E\n",
      "1 ->  \n",
      "37 -> M\n"
     ]
    }
   ],
   "source": [
    "for ex in text_encoded[:5]:\n",
    "    print('{} -> {}'.format(ex, char_array[ex]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b734df7",
   "metadata": {},
   "source": [
    "**Formulating the Problem:**\n",
    "\n",
    "<img src='/Users/blaise/Documents/ML/Machine-Learning-and-Big-Data-Analytics/resource_images/text_generation.png'>\n",
    "\n",
    "- Text generation can be phrased as a classification task. As in the image shown above, we can consider the sequences shown in the left-hand box to be the input. In order to generate new text, the goal then is to design a model that can predict the next character of a given input sequence, where the input sequence represents an incomplete text. For example, after seeing \"Deep Learn,\" the model should predict \"i\" as the next character. Given that we've got 80 unique characters, this problem then becomes a multiclass classification task\n",
    "\n",
    "<img src=\"/Users/blaise/Documents/ML/Machine-Learning-and-Big-Data-Analytics/resource_images/text_generation2.png\">\n",
    "\n",
    "- Starting with a sequence of length 1 (i.e. one single letter), we can iteratively generate new text based on this multiclass classification approach as illustrated in the image above.\n",
    "\n",
    "- To implement the text generation task in pytorch, we clip the sequence length to 40. implying the input tensor x, consisits of 40 tokens. The sequence length impacts the quality of generated text. Longer sequences can result in more meaningful sentences. For shorter sequences, the model might focus on capturing individual words correctly, while ignoring the context for the most part. Although longer sequences usually result in more meaningful sentences, for long sequences, the rnn model will have problems capturing long-range dependencies. Thus in practice, finding a sweet spot and good value for the sequence length is a hyperparameter optimization problem.\n",
    "\n",
    "- As seen in the previous figure, the inputs, x, and targets, y, are offset by one character. Hence, we will split the text into chunks of size 41: the first 40 characters will form the input sequence, x, and the last 40 elements will form the target sequence, y. \n",
    "\n",
    "- We have already stored the entire encoded text in its original order in text_encoded. We will first create text chunks consisting of 41 characters each. We will further get rid of the last chunk if its shorter than 41 characters. As a result, the new chunk dataset, named text_chunks, will always contain sequences of size 41. The 41-character chunks will then be used to construct the sequence x (i.e. the input), as well as the sequence y (the target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4c91477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee7086ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "text_chunks = [text_encoded[i:i+chunk_size] for i in range(len(text_encoded)-chunk_size+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b9c7fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([44, 32, 29,  1, 37, 48, 43, 44, 29, 42, 33, 39, 45, 43,  1, 33, 43,\n",
       "        36, 25, 38, 28,  0,  0, 51, 74,  1, 34, 70, 61, 54, 68,  1, 46, 54,\n",
       "        67, 63, 54,  0,  0, 12, 19], dtype=int32),\n",
       " array([32, 29,  1, 37, 48, 43, 44, 29, 42, 33, 39, 45, 43,  1, 33, 43, 36,\n",
       "        25, 38, 28,  0,  0, 51, 74,  1, 34, 70, 61, 54, 68,  1, 46, 54, 67,\n",
       "        63, 54,  0,  0, 12, 19, 18], dtype=int32)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd96c67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1112270"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f02136ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9f978fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text_chunk = self.text_chunks[index]\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff28adad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/yzv4_kzj2yv3z5xh7lks3hpr0000gn/T/ipykernel_1151/2835289213.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
     ]
    }
   ],
   "source": [
    "seq_dataset = TextDataset(torch.tensor(text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47170db5",
   "metadata": {},
   "source": [
    "Let's take a look at some example sequences from this transformed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2dd79760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input(x):  'THE MYSTERIOUS ISLAND\\n\\nby Jules Verne\\n\\n1'\n",
      "Target (y):  'HE MYSTERIOUS ISLAND\\n\\nby Jules Verne\\n\\n18'\n",
      "\n",
      " Input(x):  'HE MYSTERIOUS ISLAND\\n\\nby Jules Verne\\n\\n18'\n",
      "Target (y):  'E MYSTERIOUS ISLAND\\n\\nby Jules Verne\\n\\n187'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (seq, target) in enumerate(seq_dataset):\n",
    "    print(' Input(x): ',\n",
    "          repr(''.join(char_array[seq])))\n",
    "    print('Target (y): ',\n",
    "          repr(''.join(char_array[target])))\n",
    "    print()\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e55b07",
   "metadata": {},
   "source": [
    "Finally - the last step in preparing the dataset is to transform this dataset to mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82e17209",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "torch.manual_seed(1)\n",
    "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc883d",
   "metadata": {},
   "source": [
    "**Building a character-level RNN model:**\n",
    "- Now that the dataset is ready, building the model will be relatively straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "02b889f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee80ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x).unsqueeze(1)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731d77d9",
   "metadata": {},
   "source": [
    "We need to have logits as the outputs of the model so that we can sample from the model predictions in order to generate new text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e1d75fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x121190b90>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(char_array)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f3203ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(80, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aa9e742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "42332cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ad517",
   "metadata": {},
   "source": [
    "The next step we have is to create a loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c30439e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d34e47",
   "metadata": {},
   "source": [
    "Now, train the model for 10k epochs and we use one batch obtained randomly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9bb835af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | loss: 4.3708\n",
      "Epoch 500 | loss: 1.3372\n",
      "Epoch 1000 | loss: 1.2515\n",
      "Epoch 1500 | loss: 1.1892\n",
      "Epoch 2000 | loss: 1.1472\n",
      "Epoch 2500 | loss: 1.1355\n",
      "Epoch 3000 | loss: 1.0908\n",
      "Epoch 3500 | loss: 1.1101\n",
      "Epoch 4000 | loss: 1.0732\n",
      "Epoch 4500 | loss: 1.0539\n",
      "Epoch 5000 | loss: 1.0865\n",
      "Epoch 5500 | loss: 1.0617\n",
      "Epoch 6000 | loss: 1.0547\n",
      "Epoch 6500 | loss: 1.0971\n",
      "Epoch 7000 | loss: 1.0661\n",
      "Epoch 7500 | loss: 1.0987\n",
      "Epoch 8000 | loss: 1.1226\n",
      "Epoch 8500 | loss: 1.0818\n",
      "Epoch 9000 | loss: 1.0698\n",
      "Epoch 9500 | loss: 1.0914\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "torch.manual_seed(1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = model.init_hidden(batch_size)\n",
    "    hidden, cell = hidden.to(device), cell.to(device)\n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "    seq_batch,target_batch = seq_batch.to(device), target_batch.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    for c in range(seq_length):\n",
    "        pred, hidden, cell = model(seq_batch[:,c], hidden, cell)\n",
    "        loss += loss_fn(pred, target_batch[:,c])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss = loss.item()/seq_length\n",
    "    if epoch%500 == 0:\n",
    "        print(f'Epoch {epoch} | loss: {loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a65d4e",
   "metadata": {},
   "source": [
    "Next, we evaluate the model to generate new text, starting with a given short string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874fcb9",
   "metadata": {},
   "source": [
    "**Evaluation Phase - Generating new text passages:**\n",
    "- The RNN model we've trained returns logits of size 80 for each unique character. These logits can be readily converted to probabilities, via the softmax function, that a particular character will be encountered as the next character. To predict the next character in the sequence, we can simply select the element with the maximum logit value, which is equivalent to selecting the character with the highest probability. However, instead of always selecting the character with the highest likelihood, we want to randomly sample from the outputs; otherwise the model will always produce the same text. PyTorch already provides a class, torch.distributions.categorical.Categorical, which we can use to draw random samples from a categorical distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "40941b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c30128de",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "logits = torch.tensor([[1.0,1.0,1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1aebe1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:  [0.33333334 0.33333334 0.33333334]\n"
     ]
    }
   ],
   "source": [
    "print('Probabilities: ', nn.functional.softmax(logits, dim=1).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2cc2edf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "m = Categorical(logits=logits)\n",
    "samples = m.sample((10,))\n",
    "print(samples.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6294691d",
   "metadata": {},
   "source": [
    "Can be seen that, with the given logits, the categories have the same probabilities (example above). so they all are equiprobable categories. Therefore, if we use a large sample size (num_samples -> infinity ), we would expect the number of occurrences of each category to reach approx 1/3 of the sample size. By changing the logits to [1,1,3], then we would expect to observe more occurrences for category 2 (when a very large number of samples is drawn from the distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7c3add6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:  [0.10650698 0.10650698 0.78698605]\n",
      "[[0]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
    "print('Probabilities: ', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
    "m = Categorical(logits=logits)\n",
    "samples = m.sample((10,))\n",
    "print(samples.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664c867f",
   "metadata": {},
   "source": [
    "Using categorical, we can generate examples based on the logits computed by the model. \n",
    "- We will define a function sample(), that receives a short starting string, and generates a new string, generated_str, which is initially set to the input string. starting_str is encoded to a sequence of integers, encoded_input. encoded_input is passed to the rnn model one character at a time to update the hidden states. The last character of encoded model to generate a new character. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d4bd4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(1,40,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1049f50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:,2].view(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5196c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, starting_str, len_generated_text=500, scale_factor=1.0):\n",
    "    encoded_input = torch.tensor(\n",
    "        [char2int[s] for s in starting_str]\n",
    "    )\n",
    "\n",
    "    encoded_input = torch.reshape(\n",
    "        encoded_input, (1, -1)\n",
    "    ).to(device)\n",
    "    generated_str = starting_str\n",
    "    model.eval()\n",
    "    hidden,cell = model.init_hidden(1)\n",
    "    hidden,cell = hidden.to(device), cell.to(device)\n",
    "\n",
    "    for c in range(len(starting_str)-1):\n",
    "        _, hidden, cell = model(\n",
    "            encoded_input[:,c].view(1), hidden, cell\n",
    "        )\n",
    "    last_char = encoded_input[:, -1]\n",
    "    for i in range(len_generated_text):\n",
    "        logits, hidden, cell = model(\n",
    "            last_char.view(1), hidden,cell\n",
    "        )\n",
    "        logits = torch.squeeze(logits,0)\n",
    "        scaled_logits = logits*scale_factor\n",
    "        m = Categorical(logits=scaled_logits)\n",
    "        last_char = m.sample()\n",
    "        generated_str += str(char_array[last_char])\n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdfc9b3",
   "metadata": {},
   "source": [
    "Generate some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9e1d046e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island was far,\n",
      "a few hundreds which had before rapidly as fact as wild no drop and entrenty against a name which ringthening about the most dire-Fash of a black current of glasses, from died a\n",
      "bridge barked at the same puriods, the “Bound are large that one aid separable down.\n",
      "\n",
      "Cyrus Harding quickly go to see in the midst of the vessel formed an outlinely consula.\n",
      "\n",
      "What belonging to shower vive-\n",
      "\n",
      "Three substance was soon, “sponderweds.”\n",
      "\n",
      "“I trying of the day,” added Neb had, after turned the man; but\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "print(sample(model, starting_str='The island'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eacdf8",
   "metadata": {},
   "source": [
    "Furthermore, to control the predictability of generated samples (i.e., generating text following the learned patterns), the logits computed by the RNN model can be interpreted as an analog to the temp in physics. Higher temp - more entropy or randomness versus more predictable behaviour at lower temps. By scaling logits with alpha < 1, the probabilities becomee more uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "578e9ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities before scaling:  [0.10650698 0.10650698 0.78698605]\n",
      "Probabilities after scaling with 0.5:  [0.21194156 0.21194156 0.57611686]\n",
      "Probabilities after scaling with 0.1:  [0.3104238  0.3104238  0.37915248]\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
    "\n",
    "print('Probabilities before scaling: ', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
    "\n",
    "print('Probabilities after scaling with 0.5: ', nn.functional.softmax(0.5*logits, dim=1).numpy()[0])\n",
    "\n",
    "print('Probabilities after scaling with 0.1: ', nn.functional.softmax(0.1*logits, dim=1).numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d232e7b",
   "metadata": {},
   "source": [
    "alpha = 2.0 => more predictable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6c234f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island was necessary to employ and struck the corral. The settlers were now about the south coast, and the balloon in the meanwhile the last rest of the\n",
      "cavern with a\n",
      "way the balloon was diminished by the castaways had scarcely expected to any of the balloon and his companions formed the door. The colonists had been seen that the settlers were allowed to the windows, was brought to the sand of the sky, and he would proceed to the\n",
      "south.\n",
      "\n",
      "“And this day the day before an exploration, was all the aid of \n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "print(sample(model, starting_str='The island',scale_factor=2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f78f7f2",
   "metadata": {},
   "source": [
    "alpha = 0.5 => more randomnesss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d6eaec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The islandresses or\n",
      "a gig had notlieding thanks offy? Halpo\n",
      "Nen Now? Hemisphere.”\n",
      "\n",
      "The merespotrial was, unneclined Girinots will, but\n",
      "Parking, two cheezinglyas, it issued-decerribed\n",
      "five hend! We douds, ebbing smoutn, saw it exacrieter or, glass, largenetal one aim, on becuminory.\n",
      "\n",
      "Capean podo. ‘Hvice ingenieds, alleging nilled a bjegile; they wenking, and letc with head admiss of trabins. Abervive tof, 6 counly Pencrspas had vexed\n",
      "ofwerly, freeting edoth. The colonists found it aftalliue, “but happonb a\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "print(sample(model, starting_str='The island',scale_factor=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e642d564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island was necessary to employ the shore.\n",
      "\n",
      "The colonists were prevented to the sand were on the sand of the sea--the storm had been able to return to the sea.\n",
      "\n",
      "The next day, the 22nd of the part of the “Bonadventure” was the sea.\n",
      "\n",
      "The settlers looked at the same sole and soon became more difficult to give the sea.\n",
      "\n",
      "The sailor and the sailor and Neb and Pencroft and his companions should be enough to rest and on the same moment of two way to the south coast of the cavern which had landed on the sand\n",
      "of\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "print(sample(model, starting_str='The island',scale_factor=2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2389d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
