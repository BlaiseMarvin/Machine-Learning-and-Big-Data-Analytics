{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed65036",
   "metadata": {},
   "source": [
    "Recap on the fine grained operation of RNNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eea00f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ba89ff0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f60ede14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca44d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer = nn.RNN(input_size=5, hidden_size=2, num_layers=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9772bbc1",
   "metadata": {},
   "source": [
    "Inspecting the rnn layer's variables - weights in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9092194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets visualise the variables (weights in this case) of this rnn layer:\n",
    "w_xh = rnn_layer.weight_ih_l0 # input-hidden weights for layer zero\n",
    "w_hh = rnn_layer.weight_hh_l0 # hidden-hidden weights for layer zero\n",
    "b_xh = rnn_layer.bias_ih_l0 # input-hidden bias for layer zero\n",
    "b_hh = rnn_layer.bias_hh_l0 # hidden-hidden bias for layer zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3dd60e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_xh shape (input to hidden weights): torch.Size([2, 5])\n",
      "w_hh shape (hidden to hidden weights): torch.Size([2, 2])\n",
      "b_xh shape (input to hidden bias): torch.Size([2])\n",
      "b_hh shape (hidden to hidden bias): torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(f'w_xh shape (input to hidden weights): {w_xh.shape}')\n",
    "print(f'w_hh shape (hidden to hidden weights): {w_hh.shape}')\n",
    "print(f'b_xh shape (input to hidden bias): {b_xh.shape}')\n",
    "print(f'b_hh shape (hidden to hidden bias): {b_hh.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d37aad6",
   "metadata": {},
   "source": [
    "The weight and bias shapes make sense, considering the defined layer shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52448beb",
   "metadata": {},
   "source": [
    "Inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c467509d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_seq = torch.tensor([[1.0]*5, [2.0]*5, [3.0]*5]).float()\n",
    "x_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce04fa33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding a batch size\n",
    "x_seq = x_seq.unsqueeze(0)\n",
    "x_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d33ed619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 2]) torch.Size([1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "output, hn = rnn_layer(x_seq) # outputs: the output (outputs at each timestep for the model), and the final hidden state (which is the output at the final timestep)\n",
    "print(output.shape, hn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969d956",
   "metadata": {},
   "source": [
    "Now we work out the RNN manually - doing the unrolling and everything else manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35002888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 0=>\n",
      "Input:  [[1. 1. 1. 1. 1.]]\n",
      "Hidden:  [[-0.47019297  0.58639044]]\n",
      "Timestep: 1=>\n",
      "Input:  [[2. 2. 2. 2. 2.]]\n",
      "Hidden:  [[-0.8888316  1.2364398]]\n",
      "Timestep: 2=>\n",
      "Input:  [[3. 3. 3. 3. 3.]]\n",
      "Hidden:  [[-1.3074702  1.8864892]]\n"
     ]
    }
   ],
   "source": [
    "out_man = []\n",
    "\n",
    "# loop over the sequence length\n",
    "for t in range(3):\n",
    "    # at each timestep, get the input at that timestep\n",
    "    xt = x_seq[:, t, :]\n",
    "    print(f\"Timestep: {t}=>\")\n",
    "    print(\"Input: \",xt.numpy())\n",
    "\n",
    "    ht = torch.matmul(xt, torch.transpose(w_xh, 0, 1)) + b_xh # multiply and convert the input shape to hidden shape\n",
    "    print('Hidden: ',ht.detach().numpy())\n",
    "    if t>0:\n",
    "        prev_h = out_man[t-1] # if we aren't at the final timestep, then get the previous timestep's hidden state which is the previous timestep's output\n",
    "    else:\n",
    "        prev_h = torch.zeros((ht.shape))\n",
    "    ot = ht + torch.matmul(prev_h, torch.transpose(w_hh, 0, 1)) + b_hh\n",
    "    ot = torch.tanh(ot)\n",
    "    out_man.append(ot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc0284",
   "metadata": {},
   "source": [
    "comparing this with the output computed with the rnn_layer automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5da4d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn layer final hidden state:  tensor([[[-0.8649,  0.9047]]], grad_fn=<StackBackward0>)\n",
      "manually computed final hidden state:  tensor([[-0.8649,  0.9047]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# first principle is that the final hidden states should be the same:\n",
    "print(\"rnn layer final hidden state: \", hn)\n",
    "print(\"manually computed final hidden state: \", out_man[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f96ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_man2 = tuple(map(lambda x: x.detach(), out_man))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acbd256a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_man2[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57d4d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_man3 = torch.stack(out_man2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c5fe31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Layer, timestep: 1: tensor([[-0.3520,  0.5253]], grad_fn=<SelectBackward0>)\n",
      "Manual output, timestep: 1: tensor([[-0.3520,  0.5253]])\n",
      "\n",
      "RNN Layer, timestep: 2: tensor([[-0.6842,  0.7607]], grad_fn=<SelectBackward0>)\n",
      "Manual output, timestep: 2: tensor([[-0.6842,  0.7607]])\n",
      "\n",
      "RNN Layer, timestep: 3: tensor([[-0.8649,  0.9047]], grad_fn=<SelectBackward0>)\n",
      "Manual output, timestep: 3: tensor([[-0.8649,  0.9047]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"RNN Layer, timestep: {i+1}: {output[:,i,:]}\")\n",
    "    print(f\"Manual output, timestep: {i+1}: {out_man3[:,i,:]}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e6742",
   "metadata": {},
   "source": [
    "Same outputs and same hidden layer. \n",
    "Now, making the code cleaner -> implementing the rnn functionality but this time using linear layers instead of doing the matrix multiplication manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "848be2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_hidden = nn.Linear(5,2)\n",
    "hidden_hidden = nn.Linear(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b69c4f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_hidden.weight.data = w_xh\n",
    "input_hidden.bias.data = b_xh\n",
    "hidden_hidden.weight.data = w_hh\n",
    "hidden_hidden.bias.data = b_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a97550cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_manx = []\n",
    "\n",
    "for t in range(3):\n",
    "    xt = x_seq[:, t, :]\n",
    "    ht = input_hidden(xt)\n",
    "    if t>0:\n",
    "        prev_out = out_manx[t-1]\n",
    "    else:\n",
    "        prev_out = torch.zeros((ht.shape))\n",
    "    \n",
    "    ot = ht + hidden_hidden(prev_out)\n",
    "    out_manx.append(torch.tanh(ot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9714ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_manx2 = tuple(map(lambda x: x.detach(), out_manx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc309172",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_manx2 = torch.stack(out_manx2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57dab1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn layer output: tensor([[-0.3520,  0.5253]], grad_fn=<SelectBackward0>)\n",
      "manual output: tensor([[-0.3520,  0.5253]])\n",
      "rnn layer output: tensor([[-0.6842,  0.7607]], grad_fn=<SelectBackward0>)\n",
      "manual output: tensor([[-0.6842,  0.7607]])\n",
      "rnn layer output: tensor([[-0.8649,  0.9047]], grad_fn=<SelectBackward0>)\n",
      "manual output: tensor([[-0.8649,  0.9047]])\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"rnn layer output: {output[:,i,:]}\")\n",
    "    print(f\"manual output: {out_manx2[:,i,:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b979f446",
   "metadata": {},
   "source": [
    "Based off this - can create a custom rnn layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150c7fb",
   "metadata": {},
   "source": [
    "Now again making the code cleaner by using an RNN Cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34e4aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cell = nn.RNNCell(5,2) # an rnn cell with an input size of 5 and an output size of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2c89a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cell.weight_ih.data = w_xh\n",
    "rnn_cell.weight_hh.data = w_hh\n",
    "rnn_cell.bias_ih = b_xh\n",
    "rnn_cell.bias_hh = b_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5da6f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_manx2 = []\n",
    "\n",
    "for t in range(3):\n",
    "    xt = x_seq[:,t,:] # get the input at that timestep\n",
    "\n",
    "    if t > 0:\n",
    "        hidden = rnn_cell(xt, out_manx2[t-1])\n",
    "    else:\n",
    "        initial_hidden = torch.zeros((xt.shape[0],2))\n",
    "        hidden = rnn_cell(xt, initial_hidden)\n",
    "    out_manx2.append(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da4eb1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual output:  tensor([[-0.3520,  0.5253]], grad_fn=<TanhBackward0>)\n",
      "RNN layer output:  tensor([[-0.3520,  0.5253]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Manual output:  tensor([[-0.6842,  0.7607]], grad_fn=<TanhBackward0>)\n",
      "RNN layer output:  tensor([[-0.6842,  0.7607]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Manual output:  tensor([[-0.8649,  0.9047]], grad_fn=<TanhBackward0>)\n",
      "RNN layer output:  tensor([[-0.8649,  0.9047]], grad_fn=<SelectBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"Manual output: \",out_manx2[i])\n",
    "    print(\"RNN layer output: \", output[:,i,:])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d597c5d3",
   "metadata": {},
   "source": [
    "Layer Normalization applied to the RNN: we add layer normalization to the output of every timestep in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0c9ce8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = nn.LayerNorm(2)\n",
    "ln_out = []\n",
    "\n",
    "for t in range(3):\n",
    "    xt = x_seq[:, t, :] # get the input at that timestep\n",
    "\n",
    "    if t > 0:\n",
    "        hidden = rnn_cell(xt, ln_out[t-1])\n",
    "        normalized_hidden = ln(hidden)\n",
    "    else:\n",
    "        initial_hidden = torch.zeros((xt.shape[0], 2))\n",
    "        hidden = rnn_cell(xt, initial_hidden)\n",
    "        normalized_hidden = ln(hidden)\n",
    "    ln_out.append(normalized_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f527855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual output:  tensor([[-1.0000,  1.0000]], grad_fn=<NativeLayerNormBackward0>)\n",
      "RNN layer output:  tensor([[-0.3520,  0.5253]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Manual output:  tensor([[-1.0000,  1.0000]], grad_fn=<NativeLayerNormBackward0>)\n",
      "RNN layer output:  tensor([[-0.6842,  0.7607]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Manual output:  tensor([[-1.0000,  1.0000]], grad_fn=<NativeLayerNormBackward0>)\n",
      "RNN layer output:  tensor([[-0.8649,  0.9047]], grad_fn=<SelectBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"Manual output: \",ln_out[i])\n",
    "    print(\"RNN layer output: \", output[:,i,:])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde316c",
   "metadata": {},
   "source": [
    "hmm.. doing it now as it is in the textbook.. normalizing before calling the activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "af5eee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "    def forward(self, x, hidden):\n",
    "        out = self.input_hidden(x) + self.hidden_hidden(hidden)\n",
    "        out = self.ln(out)\n",
    "        return torch.tanh(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "618ff30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_rnn = CustomRNNCell(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "df595578",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_rnn.input_hidden.weight.data = w_xh\n",
    "cust_rnn.input_hidden.bias.data = b_xh\n",
    "cust_rnn.hidden_hidden.weight.data = w_hh\n",
    "cust_rnn.hidden_hidden.bias.data = b_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "da0c4fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_manx2 = []\n",
    "\n",
    "for t in range(3):\n",
    "    xt = x_seq[:,t,:] # get the input at that timestep\n",
    "\n",
    "    if t > 0:\n",
    "        hidden = cust_rnn(xt, out_manx2[t-1])\n",
    "    else:\n",
    "        initial_hidden = torch.zeros((xt.shape[0],2))\n",
    "        hidden = cust_rnn(xt, initial_hidden)\n",
    "    out_manx2.append(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4bdf3e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual output:  tensor([[-0.7616,  0.7616]], grad_fn=<TanhBackward0>)\n",
      "RNN layer output:  tensor([[-0.3520,  0.5253]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Manual output:  tensor([[-0.7616,  0.7616]], grad_fn=<TanhBackward0>)\n",
      "RNN layer output:  tensor([[-0.6842,  0.7607]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Manual output:  tensor([[-0.7616,  0.7616]], grad_fn=<TanhBackward0>)\n",
      "RNN layer output:  tensor([[-0.8649,  0.9047]], grad_fn=<SelectBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"Manual output: \",out_manx2[i])\n",
    "    print(\"RNN layer output: \", output[:,i,:])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a64a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
