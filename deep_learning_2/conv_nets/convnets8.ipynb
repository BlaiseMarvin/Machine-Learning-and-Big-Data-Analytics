{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c8c90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2edbb3",
   "metadata": {},
   "source": [
    "**Classification and Localization:**\n",
    "\n",
    "- Localizing an object in a picture can be expressed as a regression task. to predict a bounding box around the object, a common approach is to predict the horizonatal and vertical coordinates of the object's center, as well as its height and width. (4 numbers to predict). Doesn't require much change to the model - just need to add a second dense output layer with 4 units and can be trained using the MSE loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe05f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15207ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalizationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-1])\n",
    "        self.classifier = nn.Linear(512,1)\n",
    "        self.localizer = nn.Linear(512,4)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(batch, -1)\n",
    "        class_output = self.classifier(x)\n",
    "        class_output = self.sigmoid(class_output)\n",
    "        loc_output = self.localizer(x)\n",
    "        return class_output, loc_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dc6d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LocalizationModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab715a3c",
   "metadata": {},
   "source": [
    "Problem:\n",
    "- Our original dataset doesn't have bounding boxes around the images - so we need to add them ourselves. To annotate the images with bounding boxes - may want to use an opensource image labelling tool like VGG Image annotator, LabelImg, OpenLabeler, or ImgLab. or perphaps a commercial tool like LabelBox or Supervisely. Can also look into crowdsourcing platforms like Amazon Mechanical Turk if you want to have a very large number of images to annotate - <a href=\"https://arxiv.org/pdf/1611.02145\">Paper Reference</a>. \n",
    "\n",
    "- Now suppose you;ve obtained the bounding boxes for every image in the dataset (assume a single bounding box per image for now). Now need to create a dataste whose items will be batches of preprecessed images along with their class labels and bounding boxes. Each item should be a tuple of form (imagesm (class_labels, bounding boxes))\n",
    "\n",
    "- Bounding boxes should be normalized so that the horizontal and vertical coordinates, as well as the height and width all range from 0 to 1.\n",
    "\n",
    "- The MSE often works fairly well but isn't  agreat metric to evaluate how well the model can predict bounding boxes. Most common metric is the Intersection over union (IOU): the area of overlap between the predicted bounding box and the target bounding box, divided by the area of their union\n",
    "\n",
    "- <img src=\"../../data/report_images/iou.png\" alt=\"IOU Image\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab8b6a8",
   "metadata": {},
   "source": [
    "**Object Detection:**\n",
    "- Task of classifying and localizing multiple objects in an image - object detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12447a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
