{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ef9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d7de9d",
   "metadata": {},
   "source": [
    "Imaging a depth wise max pooling layer - where instead of perfoming pooling spatial wise - we actually perform it depth wise. This can allow the cnn to learn to be invariant to various features. (pick out the most important ones). For example, it could learn multiple filters, each detecting a different rotation of the same pattern and depthwise maxpooling layer would ensure that the output is the same regardless of rotation. The CNN could learn to be invariant to anything: thickness, brightness, skew, color, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a8592c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f708c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = 2\n",
    "c =torch.chunk(a, chunks = groups, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5b1dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ = tuple(map(lambda x: torch.max(x, 0, keepdim=True).values,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1607e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[ 7,  8,  9],\n",
      "        [10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "for i in c:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "049c19cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 5, 6]])\n",
      "tensor([[10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "for i in c_:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4afcc5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  5,  6],\n",
       "        [10, 11, 12]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(c_,dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb00601e",
   "metadata": {},
   "source": [
    "So logically speaking - depthwise pooling would involve splitting the tensor into groups, finding the max for each group and then concatenating the remainder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d12900fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthPool(nn.Module):\n",
    "    def __init__(self, pool_size=2):\n",
    "        super().__init__()\n",
    "        self.pool_size = pool_size\n",
    "    def forward(self, inputs):\n",
    "        # inputs are assumed to be of shape NxCxHxW\n",
    "        # instead of sliding the window across the feature maps using the pool size, we now use the pool size to reduce the\n",
    "        # channels using depth wise pooling\n",
    "        # if we wanted to do the strides - we could just call in nn.MaxPool2d with the same pool size\n",
    "        inputs = nn.functional.max_pool2d(inputs, kernel_size= self.pool_size)\n",
    "        in_channels = inputs.shape[1]\n",
    "        groups = in_channels // self.pool_size\n",
    "        chunks = torch.chunk(inputs, chunks=groups, dim=1)\n",
    "        # loop through each chunk and get the max feature map for each group\n",
    "        chunks = tuple(map(lambda x: torch.max(x, dim=1, keepdim=True).values,chunks))\n",
    "        # maximum along the channel dimension is taken for each chunk, now we need to concatenate the results into one tensor\n",
    "        return torch.cat(chunks, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7c66308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 7, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee1264e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = DepthPool(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eeb2936c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 16, 16])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb0c83",
   "metadata": {},
   "source": [
    "So, thats depthwise pooling implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecd9ce9",
   "metadata": {},
   "source": [
    "Another common pooling layer in modern conv net architectures is global average pooling. all it does id compute the mean of each entire feature map (its like an average pooling layer using a pooling kernel with the same spatial dimensions as the inputs). This means that it just outputs a single number per feature map and per instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6c917b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAveragePooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, inputs):\n",
    "        n,c,h,w = inputs.shape\n",
    "        return nn.functional.avg_pool2d(inputs,kernel_size=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37d765df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 7, 32, 32)\n",
    "global_pool = GlobalAveragePooling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2bd12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 1, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "global_pool(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef7344",
   "metadata": {},
   "source": [
    "**CNN Architectures:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3462d3",
   "metadata": {},
   "source": [
    "A common mistake is to use convolution kernels that are too large. for example, instead of using a conv layer with a 5x5 kernel, stack 2 layers with 3x3 kernels. one exception is for the first conv layer, it can typically have a large kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fcb3b9",
   "metadata": {},
   "source": [
    "**LeNet-5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed7bdf",
   "metadata": {},
   "source": [
    "**AlexNet:**\n",
    "- first to stack conv layers directly on top of one another instead of stacking a pooling layer immediately after each conv layer\n",
    "- To reduce overfitting the authors used dropout (50% dropout rate) for the fc layers, and data augmentation.\n",
    "- Data augmentation artificially increases the size of the training set by generating many realistic variants of each training instance. This reduces overfitting making this a regularization technique. The generated instances should be as realistic as possible, ideally, given an image from the augmented training set, a human should not be able to tell whether it was augmented or not. \n",
    "- Alexnet also uses a competitive normalization step immediately after the relu steps of the first 2 conv layers. called the local response normalization (LRN): the most strongly activated neurons inhibit other neurons located at the same position in neighbouring feature maps. local response normalization encourages different feature maps to specialize, pushing them apart and forcing them to exomplre a wider range of features. ultimately, improving generalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7535b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
