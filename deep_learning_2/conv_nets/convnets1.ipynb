{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33fd4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe65602",
   "metadata": {},
   "source": [
    "Reading an image file:\n",
    "- when working with images, we can read images into numpy arrays using the uint8 (unsigned 8-bit integer) data type to reduce memory usage compared to 16-bit, 32-bit, or 64-bit integer types, for example:\n",
    "- Unsigned 8-bit integers take values in the range [0,255], which is sufficient to store the pixel information in RGB images, which also take values in the same range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e3249f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e25c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = read_image('../../data/cat_dog_images/dog-01.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed3e634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  torch.Size([3, 800, 1200])\n"
     ]
    }
   ],
   "source": [
    "print('Image shape: ',img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b63c4ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels:  3\n"
     ]
    }
   ],
   "source": [
    "print('Number of channels: ',img.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d48b4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image data type:  torch.uint8\n"
     ]
    }
   ],
   "source": [
    "print('Image data type: ',img.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49125a73",
   "metadata": {},
   "source": [
    "Note: with torchvision, the input and output tensors are in the format of Tensor[channels, image_height, image_width]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7e5d8a",
   "metadata": {},
   "source": [
    "For multiple channel images, the convolutional operation is performed separately for each channel and the results are added together using matrix summation. The convolution associated with each channel c has its own kernel matrix as W[:,:,c]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470fc7f1",
   "metadata": {},
   "source": [
    "**Regularizing a NN with L2 regularization and Dropout:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b143e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b672db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1054)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "loss = loss_fn(torch.tensor([0.9]), torch.tensor([1.0]))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40555fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_lambda = 0.001\n",
    "conv_layer = nn.Conv2d(in_channels = 3,\n",
    "                       out_channels = 5,\n",
    "                       kernel_size = 5)\n",
    "\n",
    "l2_penalty = l2_lambda * sum(\n",
    "    [(p**2).sum() for p in conv_layer.parameters()]\n",
    ")\n",
    "\n",
    "loss_with_penalty = loss + l2_penalty\n",
    "linear_layer = nn.Linear(10,16)\n",
    "l2_penalty = l2_lambda * sum(\n",
    "    [(p**2).sum() for p in linear_layer.parameters()]\n",
    ")\n",
    "loss_with_penalty = loss + l2_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebb4a9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 5, 5])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# conv layer closer examination:\n",
    "conv_layer = nn.Conv2d(in_channels = 3, out_channels=5, kernel_size = 5)\n",
    "\n",
    "for param in conv_layer.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a29b2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "lin_layer = nn.Linear(10, 16)\n",
    "for param in lin_layer.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7ee82d",
   "metadata": {},
   "source": [
    "Weight decay v. L2 Regularization:\n",
    "- an alternative way to use L2 regularization is by setting the weight decay parameter in a PyTorch optimizer to a positive value. For example:\n",
    "- while L2 regularization and weight_decay are not strictly identical, it can be shown that they are equivalent when using stochastic gradient descent (SGD) optimizers. \n",
    "- can use dropout as well, which is usually applied to the hidden units of higher layers. During the training phase, of an NN, a fraction of the hidden units are randomly dropped at every iteration with the probability pdrop. This dropout probability is determined by the yser and the common choice is p=0.5. When dropping a certain fraction of input neurons, the weights associated with the remaining neurons are rescaled to account for the missing (dropped) neurons.\n",
    "- The effect is that the network os forced to learn a redundant representation of the data. Therefore, the network cannot rely on the activation of any set of hidden units, since they may be turned off at any time during training, and is forced to learn more general and robust patterns from the data. \n",
    "- random dropout can effectively prevent overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b5d11f",
   "metadata": {},
   "source": [
    "**Loss functions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4061ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE (w Probas): 0.3711\n",
      "BCE (w Logits): 0.3711\n"
     ]
    }
   ],
   "source": [
    "# binary cross-entropy\n",
    "logits = torch.tensor([0.8])\n",
    "probas = torch.sigmoid(logits)\n",
    "target = torch.tensor([1.0])\n",
    "bce_loss_fn = nn.BCELoss() # inputs to the returned object are pred(probabilities), target\n",
    "bce_logits_loss_fn = nn.BCEWithLogitsLoss() # inputs to the returned object are logits, target\n",
    "print(f'BCE (w Probas): {bce_loss_fn(probas, target):.4f}')\n",
    "print(f'BCE (w Logits): {bce_logits_loss_fn(logits, target):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "801b9959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCE (w Logits): 0.5996\n",
      "CCE (w Probas): 0.5996\n"
     ]
    }
   ],
   "source": [
    "## Categorical cross entropy loss\n",
    "logits = torch.tensor([[1.5, 0.8, 2.1]])\n",
    "probas = torch.softmax(logits, dim=1)\n",
    "target = torch.tensor([2])\n",
    "cce_loss_fn = nn.NLLLoss()\n",
    "cce_logits_loss_fn = nn.CrossEntropyLoss()\n",
    "print(f'CCE (w Logits): {cce_logits_loss_fn(logits, target):.4f}')\n",
    "print(f'CCE (w Probas): {cce_loss_fn(torch.log(probas),target):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb13bb0",
   "metadata": {},
   "source": [
    "**Implementing a deep CNN using PyTorch:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcb6c2f",
   "metadata": {},
   "source": [
    "**Loading and preprocessing the data:**\n",
    "- First, we load the MNIST dataset using the torchvision module and construct the training and test sets, as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b1597c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision \n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf5a2d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcdebae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb80d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '../../data/mnist'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "mnist_dataset = torchvision.datasets.MNIST(root=image_path, train=True, transform=transform, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf62a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_valid_dataset = Subset(mnist_dataset, torch.arange(10000))\n",
    "mnist_train_dataset = Subset(mnist_dataset, torch.arange(10000, len(mnist_dataset)))\n",
    "mnist_test_dataset = torchvision.datasets.MNIST(\n",
    "    root = image_path, train=False, transform=transform, download=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86065e5b",
   "metadata": {},
   "source": [
    "Next, we construct the data loader with batches of 64 images for the training set and validation set respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39fa5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "\n",
    "train_dl = DataLoader(mnist_train_dataset, batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(mnist_valid_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78143907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
