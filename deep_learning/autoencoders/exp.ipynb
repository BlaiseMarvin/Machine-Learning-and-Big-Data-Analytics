{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93b55c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blais\\Documents\\ML\\venv2\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\blais\\Documents\\ML\\venv2\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling KLDivergenceLayer.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'kl_divergence_layer' (of type KLDivergenceLayer). Either the `KLDivergenceLayer.call()` method is incorrect, or you need to implement the `KLDivergenceLayer.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nmodule 'keras.backend' has no attribute 'sum'\u001b[0m\n\nArguments received by KLDivergenceLayer.call():\n  • args=(['<KerasTensor shape=(None, 2), dtype=float32, sparse=False, ragged=False, name=keras_tensor_5>', '<KerasTensor shape=(None, 2), dtype=float32, sparse=False, ragged=False, name=keras_tensor_6>'],)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m z_mu \u001b[38;5;241m=\u001b[39m Dense(latent_dim)(h)\n\u001b[0;32m     60\u001b[0m z_log_var \u001b[38;5;241m=\u001b[39m Dense(latent_dim)(h)\n\u001b[1;32m---> 62\u001b[0m z_mu, z_log_var \u001b[38;5;241m=\u001b[39m \u001b[43mKLDivergenceLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mz_mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_log_var\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m z_sigma \u001b[38;5;241m=\u001b[39m Lambda(\u001b[38;5;28;01mlambda\u001b[39;00m t: K\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m.5\u001b[39m\u001b[38;5;241m*\u001b[39mt))(z_log_var)\n\u001b[0;32m     65\u001b[0m eps \u001b[38;5;241m=\u001b[39m Input(tensor\u001b[38;5;241m=\u001b[39mK\u001b[38;5;241m.\u001b[39mrandom_normal(stddev\u001b[38;5;241m=\u001b[39mepsilon_std,\n\u001b[0;32m     66\u001b[0m                                    shape\u001b[38;5;241m=\u001b[39m(K\u001b[38;5;241m.\u001b[39mshape(x)[\u001b[38;5;241m0\u001b[39m], latent_dim)))\n",
      "File \u001b[1;32mc:\\Users\\blais\\Documents\\ML\\venv2\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[1], line 42\u001b[0m, in \u001b[0;36mKLDivergenceLayer.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     40\u001b[0m     mu, log_var \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m---> 42\u001b[0m     kl_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m log_var \u001b[38;5;241m-\u001b[39m\n\u001b[0;32m     43\u001b[0m                             K\u001b[38;5;241m.\u001b[39msquare(mu) \u001b[38;5;241m-\u001b[39m\n\u001b[0;32m     44\u001b[0m                             K\u001b[38;5;241m.\u001b[39mexp(log_var), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_loss(K\u001b[38;5;241m.\u001b[39mmean(kl_batch), inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "\u001b[1;31mAttributeError\u001b[0m: Exception encountered when calling KLDivergenceLayer.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'kl_divergence_layer' (of type KLDivergenceLayer). Either the `KLDivergenceLayer.call()` method is incorrect, or you need to implement the `KLDivergenceLayer.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nmodule 'keras.backend' has no attribute 'sum'\u001b[0m\n\nArguments received by KLDivergenceLayer.call():\n  • args=(['<KerasTensor shape=(None, 2), dtype=float32, sparse=False, ragged=False, name=keras_tensor_5>', '<KerasTensor shape=(None, 2), dtype=float32, sparse=False, ragged=False, name=keras_tensor_6>'],)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Add, Multiply\n",
    "from keras.models import Model, Sequential\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "original_dim = 784\n",
    "intermediate_dim = 256\n",
    "latent_dim = 2\n",
    "batch_size = 100\n",
    "epochs = 50\n",
    "epsilon_std = 1.0\n",
    "\n",
    "\n",
    "def nll(y_true, y_pred):\n",
    "    \"\"\" Negative log likelihood (Bernoulli). \"\"\"\n",
    "\n",
    "    # keras.losses.binary_crossentropy gives the mean\n",
    "    # over the last axis. we require the sum\n",
    "    return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "\n",
    "class KLDivergenceLayer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) -\n",
    "                                K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(K.mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "\n",
    "decoder = Sequential([\n",
    "    Dense(intermediate_dim, input_dim=latent_dim, activation='relu'),\n",
    "    Dense(original_dim, activation='sigmoid')\n",
    "])\n",
    "\n",
    "x = Input(shape=(original_dim,))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "z_mu = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "z_mu, z_log_var = KLDivergenceLayer()([z_mu, z_log_var])\n",
    "z_sigma = Lambda(lambda t: K.exp(.5*t))(z_log_var)\n",
    "\n",
    "eps = Input(tensor=K.random_normal(stddev=epsilon_std,\n",
    "                                   shape=(K.shape(x)[0], latent_dim)))\n",
    "z_eps = Multiply()([z_sigma, eps])\n",
    "z = Add()([z_mu, z_eps])\n",
    "\n",
    "x_pred = decoder(z)\n",
    "\n",
    "vae = Model(inputs=[x, eps], outputs=x_pred)\n",
    "vae.compile(optimizer='rmsprop', loss=nll)\n",
    "\n",
    "# train the VAE on MNIST digits\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, original_dim) / 255.\n",
    "x_test = x_test.reshape(-1, original_dim) / 255.\n",
    "\n",
    "vae.fit(x_train,\n",
    "        x_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92522ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
