{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "768db4c5",
   "metadata": {},
   "source": [
    "Here's some commonly faced problems when building deep neural nets:\n",
    "- Vanishing and Exploding gradients\n",
    "- Might not have enough training data for such a large network, or it might be too costly to label\n",
    "- Training may be extremely slow\n",
    "- A model with millions of parameters would severely risk overfitting the training set, especially if there are not enough training instances or if they're too noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefe3f12",
   "metadata": {},
   "source": [
    "**The Vanishing/Exploding Gradients Problem:**\n",
    "- Unfortunately gradients often get smaller and smaller as the algorithm progresses down to the lower layers (chain rule - multiplying multiple parameters - vanishing gradients) - as a result the gradient descent update leaves the lower layers' connection weights virtually unchanged, and training never converges to a good solution - this is called the vanishing gradients problem. Reverse can also happen - gradients exploding.\n",
    "- More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds\n",
    "- Few suspects were found for this problem earlier on - the combination of the sigmoid (logistic) activation function and the weight initialization technique that was most popular at the tume (i.e. a normal distribution with a mean of 0 and a standard deviation of 1) - with this activation function and this initialization scheme the variance of the outputs of each layer is much greater than the variance of the of the inputs. \n",
    "  - Glorot and He Initialization:\n",
    "    - Glorot and Bengio propose a way to significantly alleviate the unstable gradients problem. They point out that we need the signal to flow properly in both directions: the forward direction when making predictions and the reverse direction when backpropagating gradients. We don't want the signal to die out, nor do we want it to explode and saturate. For the signal to flow properly,  the authors argue we need the variance of the outputs of each layer to be equal to the variance of its inputs and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction. Its actually not possible to guarantee both unless the layer has an equal number of inputs and outputs but Glorot and Bengio proposed a good compromise that has proven to work well in practice: the connection weights of each layer must be initialized randomly where fanavg = (fanin + fanout)/2. \n",
    "    - Lesson here is that based on these guys' research there's different initialization strategies preferred for various activation functions.\n",
    "      - We have Glorot initialization preferred for tanh, sigmoid, and softmax\n",
    "      - He initialization preferred for ReLU, Leaky ReLU, ELU, GELU, Swish, Mish\n",
    "      - LeCun initialization - SELU\n",
    "    - By default keras uses Glorot initialization with a uniform distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c10040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28cfbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63940df6",
   "metadata": {},
   "source": [
    "Alternatively - you can obtain any of the initializations l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a26346",
   "metadata": {},
   "source": [
    "**Better Activation Functions:**\n",
    "- One of the insights in the 2010 paper was the problems with unstable gradients were in part due to a poor choice of activation function.\n",
    "- Turns out other activation functions perform much better than sigmoid activations in deep neural networks - in particular the ReLU activation function - mostly because it does not saturate for positive values, and also because it is very fast to compute\n",
    "- Unfortuanately, ReLU activation functions are not perfect - they suffer from a problem known as dying ReLUs: during training, some neurons effectively die meaning they stop outputting anything other than 0. A neuron dies when its weights get tweaked in such a way that the input of the ReLU function (i.e. the weighted sum of the neuron's inputs plus its bias term) is negative for all instances in the training set. When this happens it just keeps outputting zeros - and gradient descent - does not affect it anymore because the gradient of the relu finction is zero when its input is negative. \n",
    "- To solve solve this problem - you may want to use a variant of the ReLU function - such as leaky ReLU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f529b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0995e4c",
   "metadata": {},
   "source": [
    "**Leaky ReLU**\n",
    "- The leaky relu activation function - is defined as leak\n",
    "- Setting alpha to 0.2, a huge leak seemed to result in better performance than alpha = 0.01 (small leak). \n",
    "- There's also the parametric leaky relu where alpha is authorised to be learned during training: instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter. Paraametric  Leaky ReLU was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting on the training set.\n",
    "- Keras includes the classes LeakyReLU and PReLU in the tf.keras.layers package. Just like for other relu variants - use the he initialization with these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8f5b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blais\\Documents\\ML\\venv2\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2) # defaults to alpha=0.3\n",
    "dense = tf.keras.layers.Dense(50, activation=leaky_relu, kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76368507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<LeakyReLU name=leaky_re_lu, built=True>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf113a4",
   "metadata": {},
   "source": [
    "if you prefer, you can also use leaky relu as a separate layer in your model; it makes no difference for training and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc767482",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    ..... # more layers\n",
    "    tf.keras.layers.Dense(50, kernel_initializer=\"he_normal\"), # no activation\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.2), #activation as a separate layer\n",
    "    ....... # more layers\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ded757",
   "metadata": {},
   "source": [
    "For PReLU, replace LeakyReLU with PReLU. currently, there's no official implementation of RReLU in keras - but you can fairly easily implement your own. \n",
    "- ReLU, leaky ReLU, and PReLU all suffer from the fact that they are not smooth functions: their derivatives abruptly change at (z=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd65310",
   "metadata": {},
   "source": [
    "**ELU and SELU**\n",
    "- New activation function => called exponential linear unit (ELU), outperforming all ReLU variants "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83a0285",
   "metadata": {},
   "source": [
    "**Batch Normalization:**\n",
    "- learns the optimal scaling and mean parameter to standardize your data\n",
    "- during training, BN standardizes its inputs, then it rescales and offsets them. \n",
    "- In the initial research => \n",
    "  - discovered that batch normalization considerably improved all the deep neural networks they experimented with, leading to a huge improvement in the imagenet classification task. Batch norm solves the vanishing gradients problem, can even use saturating activation functions, networks much less sensitive to weights initialization.\n",
    "  - Batch norm also acts like a regularizer - reducing the need for other regularization techniques.\n",
    "  - Batch norm however adds some complexity to the model - moreover there is a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "031a72c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=[28,28]),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3aa4dd",
   "metadata": {},
   "source": [
    "batch norm in the above is used even early on at the input stage to standardize the data - eliminating the need for a normalization/standardscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dde05934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │         \u001b[38;5;34m3,136\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │         \u001b[38;5;34m1,200\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">271,346</span> (1.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m271,346\u001b[0m (1.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,978</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m268,978\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,368</span> (9.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,368\u001b[0m (9.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1986c",
   "metadata": {},
   "source": [
    "Lets look at the parameters of the first BN layer. 2 are trainable by backprop and 2 are not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa97a5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gamma', True),\n",
       " ('beta', True),\n",
       " ('moving_mean', False),\n",
       " ('moving_variance', False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38aeac",
   "metadata": {},
   "source": [
    "Can experiment with adding batch norm before and after the activation fxn to see where it performs best for a given dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74147af",
   "metadata": {},
   "source": [
    "**Reusing Pretrained Layers:**\n",
    "- Transfer Learning\n",
    "- Speeds up training considerably but also requires significantly less training data\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f6a4c6",
   "metadata": {},
   "source": [
    "**Transfer learning in keras:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dec0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first you need to load the original model and create a new model based on its layers\n",
    "# here say, you decide to reuse all the layers except for the output layer:\n",
    "\n",
    "model_A = tf.keras.load_model(\"my_model_A\")\n",
    "\n",
    "model_B_on_A = tf.keras.Sequential(model_A.layers[:-1])\n",
    "\n",
    "model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef293027",
   "metadata": {},
   "source": [
    "Note - model_A and model_B_on_A - now share some layers. When you train model_B_on_A, it will also affect model_A. To avoid that - clone model_A before you reuse its layers. clone the model then copy its weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449b993",
   "metadata": {},
   "source": [
    "model_A_clone = tf.keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a0e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbe0af6",
   "metadata": {},
   "source": [
    "Now, you can train the model for a few epochs - then unfreeze the reused layers (which requires compiling again), and continue training to fine-tune the resued layers for task B. after unfreezing - good practive to reduce the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75866c1",
   "metadata": {},
   "source": [
    "Transfer learning usually only works with really deep neural nets.\n",
    "If there isn't a suitable model you can use for transfer learning - another effective technique is\n",
    "- Use Autoencoders or GANS - reuse the lower layers - add output layers for your task and finetune the model.\n",
    "- Could also try pretraining on an auxiliary task for which its easy to obtain labels and then finetune on your final task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4dc16c",
   "metadata": {},
   "source": [
    "**Learning Rate Scheduling:**\n",
    "- based off experiments - preferred lr scheduling algorithms are power scheduling, exponential scheduling and 1cycle scheduling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357d45d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blais\\Documents\\ML\\venv2\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# implementing power scheduling in keras is the easiest option-just set the decay hyperparam when creating an optimizer\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, decay=1.0e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e27f692",
   "metadata": {},
   "source": [
    "Avoiding Overfitting through regularization:\n",
    "- already implemented the best regularization technique - early stopping => Moreover, even though batch normalization was designed to solve the unstable gradients problems, it also acts like a good regularizer. L1 and L2 regularization, dropout, and max-norm regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3202c5",
   "metadata": {},
   "source": [
    "**L1 and L2 Regularization:**\n",
    "- L2 Regularization to constrain a neural net's connection weights, and or l1 regularization if you want a sparse model (with many weights equal to 0). Here's how to apply l2 regularization to a keras layer's connection weights, using a regularization factor of 0.01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aad2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "layer = tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\", kernel_regularizer=tf.keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1fd84",
   "metadata": {},
   "source": [
    "if you want l1 regularization - tf.keras.regularizers.l1() - and if you want both => tf.keras.regularizers.l1_l2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9baaca",
   "metadata": {},
   "source": [
    "l2 regularization is fine when using sgd, momentum optimization, and nesterov momentum optimization, but not with adam and its variants. if you want to use adam with weight decay, then do not use l2 regularization - use AdamW instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74044121",
   "metadata": {},
   "source": [
    "**Dropout:**\n",
    "- Dropout's one of the most popular regularization techniques for deep neural networks. \n",
    "- Fairly simple algorithm - at every training step - every neuron including the input neurons but excluding the output neurons has a probability p of being temporarily dropped out - meaning it will be entirely ignored during training but may be active during the next. Hyperparameter p is called the dropout rate and is typocally set between 10 and 50% closer to 20-30% in recurrent neural nets and closer to 40-50% in conv nets.\n",
    "- Since dropout is only active during training, comparing the training loss and validation loss can be misleading. In particular, a model may be overfitting the training set and yet have similar training and validation losses. So, make sure to eval the training loss after w/o dropout - i.e. after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d84ebfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=[28,28]),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f6951",
   "metadata": {},
   "source": [
    "**Monte Carlo (MC) Dropout:**\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d26473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
